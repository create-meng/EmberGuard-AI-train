"""
å¿«é€Ÿæµ‹è¯• - æµ‹è¯•LSTMæ¨¡å‹åœ¨æµ‹è¯•å›¾ç‰‡ä¸Šçš„æ•ˆæœ
"""
import sys
from pathlib import Path
import cv2
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from emberguard.feature_extractor import FeatureExtractor
from emberguard.lstm_model import LSTMTrainer
from ultralytics import YOLO


def quick_test():
    """å¿«é€Ÿæµ‹è¯•LSTMæ¨¡å‹"""
    print("\n" + "ğŸ”¥" * 30)
    print("EmberGuard AI - LSTMå¿«é€Ÿæµ‹è¯•")
    print("ğŸ”¥" * 30)
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    yolo_model = YOLO('runs/detect/train2/weights/best.pt')
    lstm_model = LSTMTrainer.load_model('models/lstm/best.pt')
    feature_extractor = FeatureExtractor()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯•å›¾ç‰‡
    test_images = [
        'test_picture/1.png',
        'test_picture/2.jpg',
        'test_picture/3.jpg',
        'test_picture/4.jpg'
    ]
    
    class_names = {0: "æ— ç«", 1: "çƒŸé›¾", 2: "ç«ç„°"}
    
    for img_path in test_images:
        if not Path(img_path).exists():
            print(f"\nâš ï¸  å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
            continue
        
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•å›¾ç‰‡: {img_path}")
        print(f"{'='*60}")
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(img_path)
        
        # YOLOæ£€æµ‹
        results = yolo_model(img, verbose=False)
        
        # æå–ç‰¹å¾
        features = feature_extractor.get_best_detection(results, img.shape)
        
        print(f"\nYOLOæ£€æµ‹:")
        if len(results[0].boxes) > 0:
            for box in results[0].boxes:
                conf = float(box.conf[0])
                cls = int(box.cls[0])
                cls_name = 'fire' if cls == 0 else 'smoke'
                print(f"  æ£€æµ‹åˆ°: {cls_name}, ç½®ä¿¡åº¦: {conf:.3f}")
        else:
            print(f"  æœªæ£€æµ‹åˆ°ç«/çƒŸ")
        
        # åˆ›å»º30å¸§åºåˆ—ï¼ˆé‡å¤å½“å‰ç‰¹å¾ï¼‰
        sequence = np.array([features] * 30)
        
        # LSTMé¢„æµ‹
        pred_class, probs = lstm_model.predict(sequence)
        pred_class = pred_class[0]
        probs = probs[0]
        
        print(f"\nLSTMé¢„æµ‹:")
        print(f"  é¢„æµ‹ç±»åˆ«: {class_names[pred_class]}")
        print(f"  ç½®ä¿¡åº¦: {probs[pred_class]:.3f}")
        print(f"  æ¦‚ç‡åˆ†å¸ƒ:")
        print(f"    æ— ç«: {probs[0]:.3f}")
        print(f"    çƒŸé›¾: {probs[1]:.3f}")
        print(f"    ç«ç„°: {probs[2]:.3f}")


def test_video_sample():
    """æµ‹è¯•è§†é¢‘é‡‡æ ·ï¼ˆä»è§†é¢‘ä¸­é‡‡æ ·30å¸§ï¼‰"""
    print("\n" + "ğŸ”¥" * 30)
    print("EmberGuard AI - è§†é¢‘é‡‡æ ·æµ‹è¯•")
    print("ğŸ”¥" * 30)
    
    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•è§†é¢‘
    test_video = "datasets/fire_videos_organized/mixed/archive_fire and smoke.mp4"
    
    if not Path(test_video).exists():
        print(f"\nâŒ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video}")
        # å°è¯•å…¶ä»–è§†é¢‘
        mixed_dir = Path("datasets/fire_videos_organized/mixed")
        videos = list(mixed_dir.glob("*.mp4")) + list(mixed_dir.glob("*.avi"))
        if videos:
            test_video = str(videos[0])
            print(f"ä½¿ç”¨: {test_video}")
        else:
            print("æœªæ‰¾åˆ°æµ‹è¯•è§†é¢‘")
            return
    
    print(f"\næµ‹è¯•è§†é¢‘: {test_video}")
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    yolo_model = YOLO('runs/detect/train2/weights/best.pt')
    lstm_model = LSTMTrainer.load_model('models/lstm/best.pt')
    feature_extractor = FeatureExtractor()
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(test_video)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}")
    
    # é‡‡æ ·30å¸§
    sample_indices = np.linspace(0, total_frames-1, 30, dtype=int)
    features_list = []
    
    print("\næå–ç‰¹å¾...")
    for idx in sample_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            break
        
        # YOLOæ£€æµ‹
        results = yolo_model(frame, verbose=False)
        
        # æå–ç‰¹å¾
        features = feature_extractor.get_best_detection(results, frame.shape)
        features_list.append(features)
    
    cap.release()
    
    if len(features_list) < 30:
        print(f"âš ï¸  åªæå–äº† {len(features_list)} å¸§")
        return
    
    # åˆ›å»ºåºåˆ—
    sequence = np.array(features_list)
    
    # LSTMé¢„æµ‹
    pred_class, probs = lstm_model.predict(sequence)
    pred_class = pred_class[0]
    probs = probs[0]
    
    class_names = {0: "æ— ç«", 1: "çƒŸé›¾", 2: "ç«ç„°"}
    
    print(f"\n{'='*60}")
    print("LSTMé¢„æµ‹ç»“æœ")
    print(f"{'='*60}")
    print(f"é¢„æµ‹ç±»åˆ«: {class_names[pred_class]}")
    print(f"ç½®ä¿¡åº¦: {probs[pred_class]:.3f}")
    print(f"\næ¦‚ç‡åˆ†å¸ƒ:")
    print(f"  æ— ç«: {probs[0]:.3f}")
    print(f"  çƒŸé›¾: {probs[1]:.3f}")
    print(f"  ç«ç„°: {probs[2]:.3f}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¿«é€Ÿæµ‹è¯•LSTMæ¨¡å‹')
    parser.add_argument('--mode', type=str, default='image', choices=['image', 'video'],
                       help='æµ‹è¯•æ¨¡å¼: image(å›¾ç‰‡) æˆ– video(è§†é¢‘é‡‡æ ·)')
    
    args = parser.parse_args()
    
    try:
        if args.mode == 'image':
            quick_test()
        else:
            test_video_sample()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
