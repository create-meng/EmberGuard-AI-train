"""
å‡†å¤‡LSTMè®­ç»ƒæ•°æ®
ä»è§†é¢‘ä¸­æå–ç‰¹å¾åºåˆ—å¹¶æ ‡æ³¨
"""
import sys
from pathlib import Path
import cv2
import numpy as np
import json
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from emberguard.feature_extractor import FeatureExtractor
from ultralytics import YOLO


class LSTMDataPreparer:
    """LSTMæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, yolo_model_path, sequence_length=30):
        """
        åˆå§‹åŒ–
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            sequence_length: åºåˆ—é•¿åº¦ï¼ˆå¸§æ•°ï¼‰
        """
        self.yolo_model = YOLO(yolo_model_path)
        self.feature_extractor = FeatureExtractor()
        self.sequence_length = sequence_length
        
    def extract_features_from_video(self, video_path, stride=1):
        """
        ä»è§†é¢‘æå–ç‰¹å¾åºåˆ—
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            stride: é‡‡æ ·æ­¥é•¿ï¼ˆæ¯éš”strideå¸§æå–ä¸€æ¬¡ï¼‰
            
        Returns:
            list: ç‰¹å¾åºåˆ—åˆ—è¡¨
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"å¤„ç†è§†é¢‘: {video_path}")
        print(f"æ€»å¸§æ•°: {total_frames}")
        
        features_list = []
        frame_idx = 0
        
        with tqdm(total=total_frames, desc="æå–ç‰¹å¾") as pbar:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æŒ‰æ­¥é•¿é‡‡æ ·
                if frame_idx % stride == 0:
                    # YOLOæ£€æµ‹
                    results = self.yolo_model(frame, verbose=False)
                    
                    # æå–ç‰¹å¾
                    features = self.feature_extractor.get_best_detection(results, frame.shape)
                    features_list.append(features)
                
                frame_idx += 1
                pbar.update(1)
        
        cap.release()
        
        print(f"æå–äº† {len(features_list)} ä¸ªç‰¹å¾å‘é‡")
        return features_list
    
    def create_sequences(self, features_list, label):
        """
        åˆ›å»ºè®­ç»ƒåºåˆ—
        
        Args:
            features_list: ç‰¹å¾åˆ—è¡¨
            label: æ ‡ç­¾ (0=æ— ç«, 1=çƒŸé›¾, 2=ç«ç„°)
            
        Returns:
            tuple: (sequences, labels)
        """
        sequences = []
        labels = []
        
        # æ»‘åŠ¨çª—å£åˆ›å»ºåºåˆ—
        for i in range(len(features_list) - self.sequence_length + 1):
            seq = features_list[i:i + self.sequence_length]
            sequences.append(seq)
            labels.append(label)
        
        return np.array(sequences), np.array(labels)
    
    def prepare_dataset(self, video_list, output_dir):
        """
        å‡†å¤‡å®Œæ•´æ•°æ®é›†
        
        Args:
            video_list: è§†é¢‘åˆ—è¡¨ [(video_path, label), ...]
            output_dir: è¾“å‡ºç›®å½•
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        all_sequences = []
        all_labels = []
        
        for video_path, label in video_list:
            print(f"\nå¤„ç†: {video_path} (æ ‡ç­¾: {label})")
            
            # æå–ç‰¹å¾
            features = self.extract_features_from_video(video_path)
            
            # åˆ›å»ºåºåˆ—
            sequences, labels = self.create_sequences(features, label)
            
            all_sequences.append(sequences)
            all_labels.append(labels)
            
            print(f"ç”Ÿæˆ {len(sequences)} ä¸ªåºåˆ—")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_sequences = np.concatenate(all_sequences, axis=0)
        all_labels = np.concatenate(all_labels, axis=0)
        
        print(f"\næ€»åºåˆ—æ•°: {len(all_sequences)}")
        print(f"åºåˆ—å½¢çŠ¶: {all_sequences.shape}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(all_labels)}")
        
        # ä¿å­˜æ•°æ®
        np.save(output_dir / 'sequences.npy', all_sequences)
        np.save(output_dir / 'labels.npy', all_labels)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'num_sequences': len(all_sequences),
            'sequence_length': self.sequence_length,
            'feature_dim': 8,
            'num_classes': 3,
            'class_names': ['æ— ç«', 'çƒŸé›¾', 'ç«ç„°'],
            'label_distribution': np.bincount(all_labels).tolist()
        }
        
        with open(output_dir / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\næ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
        print(f"- sequences.npy: {all_sequences.shape}")
        print(f"- labels.npy: {all_labels.shape}")
        print(f"- metadata.json")


def load_video_list_from_organized():
    """ä»æ•´ç†å¥½çš„ç›®å½•åŠ è½½è§†é¢‘åˆ—è¡¨"""
    from pathlib import Path
    
    base_dir = Path("datasets/fire_videos_organized")
    video_list = []
    
    # åŠ è½½ç«ç¾è§†é¢‘ï¼ˆæ ‡ç­¾2ï¼‰
    fire_dir = base_dir / "fire"
    if fire_dir.exists():
        for video_file in fire_dir.glob("*"):
            if video_file.suffix.lower() in ['.avi', '.mp4', '.mov']:
                video_list.append((str(video_file), 2))
    
    # åŠ è½½çƒŸé›¾è§†é¢‘ï¼ˆæ ‡ç­¾1ï¼‰
    smoke_dir = base_dir / "smoke"
    if smoke_dir.exists():
        for video_file in smoke_dir.glob("*"):
            if video_file.suffix.lower() in ['.avi', '.mp4', '.mov']:
                video_list.append((str(video_file), 1))
    
    # åŠ è½½æ­£å¸¸è§†é¢‘ï¼ˆæ ‡ç­¾0ï¼‰
    normal_dir = base_dir / "normal"
    if normal_dir.exists():
        for video_file in normal_dir.glob("*"):
            if video_file.suffix.lower() in ['.avi', '.mp4', '.mov']:
                video_list.append((str(video_file), 0))
    
    return video_list


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("LSTMè®­ç»ƒæ•°æ®å‡†å¤‡å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥æ•´ç†å¥½çš„æ•°æ®æ˜¯å¦å­˜åœ¨
    from pathlib import Path
    organized_dir = Path("datasets/fire_videos_organized")
    
    if not organized_dir.exists():
        print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°æ•´ç†å¥½çš„æ•°æ®ç›®å½•")
        print("è¯·å…ˆè¿è¡Œ: python scripts/organize_downloaded_data.py")
        return
    
    # åŠ è½½è§†é¢‘åˆ—è¡¨
    print("\nğŸ“‚ åŠ è½½æ•´ç†å¥½çš„è§†é¢‘æ•°æ®...")
    video_list = load_video_list_from_organized()
    
    if not video_list:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        return
    
    # ç»Ÿè®¡
    fire_count = sum(1 for _, label in video_list if label == 2)
    smoke_count = sum(1 for _, label in video_list if label == 1)
    normal_count = sum(1 for _, label in video_list if label == 0)
    
    print(f"\næ‰¾åˆ°è§†é¢‘:")
    print(f"  ç«ç¾è§†é¢‘: {fire_count}")
    print(f"  çƒŸé›¾è§†é¢‘: {smoke_count}")
    print(f"  æ­£å¸¸è§†é¢‘: {normal_count}")
    print(f"  æ€»è®¡: {len(video_list)}")
    
    # åˆå§‹åŒ–
    print("\nğŸ”§ åˆå§‹åŒ–ç‰¹å¾æå–å™¨...")
    preparer = LSTMDataPreparer(
        yolo_model_path='runs/detect/train2/weights/best.pt',
        sequence_length=30
    )
    
    # å‡†å¤‡æ•°æ®é›†
    print("\nğŸš€ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®...")
    print("è¿™å¯èƒ½éœ€è¦30-60åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print()
    
    try:
        preparer.prepare_dataset(video_list, 'datasets/lstm_data')
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
        print("=" * 60)
        print("\nä¸‹ä¸€æ­¥:")
        print("  python scripts/4_train_lstm.py --data_dir datasets/lstm_data --epochs 50")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
