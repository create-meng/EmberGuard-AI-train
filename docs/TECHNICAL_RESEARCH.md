# EmberGuard AI - æŠ€æœ¯ç ”ç©¶æŠ¥å‘Š

## ğŸ“‹ æ–‡æ¡£ä¿¡æ¯

- **é¡¹ç›®åç§°**: EmberGuard AI - æ™ºèƒ½ç«ç¾æ£€æµ‹ç³»ç»Ÿ
- **ç ”ç©¶æ—¥æœŸ**: 2026å¹´2æœˆ6æ—¥
- **ç ”ç©¶ç›®æ ‡**: åˆ†æç°æœ‰YOLO+LSTMç«ç¾/çƒŸé›¾æ£€æµ‹æ–¹æ¡ˆï¼Œåˆ¶å®šæŠ€æœ¯å®ç°è·¯çº¿
- **ç ”ç©¶æ–¹æ³•**: GitHubå¼€æºé¡¹ç›®åˆ†æã€ä»£ç å®¡æŸ¥ã€æ¶æ„å¯¹æ¯”

---

## ğŸ¯ ç ”ç©¶ç›®æ ‡

åŸºäºäº§å“è¯´æ˜ä¹¦çš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ï¼š

1. **YOLO-LSTMèåˆå¼‚å¸¸æ£€æµ‹** - ç»“åˆç›®æ ‡æ£€æµ‹ä¸æ—¶åºåˆ†æ
2. **çƒ­çº¢å¤–ç‰¹å¾èåˆ** - å¤šæ¨¡æ€æ•°æ®èåˆ
3. **æ—¶ç©ºä¸Šä¸‹æ–‡å»ºæ¨¡** - åŒºåˆ†ç‚ŠçƒŸä¸ç«ç¾çƒŸé›¾
4. **è¯¯æŠ¥ç‡<2%** - é«˜ç²¾åº¦æ£€æµ‹
5. **è¾¹ç¼˜è®¡ç®—éƒ¨ç½²** - è½»é‡åŒ–æ¨¡å‹

---

## ğŸ“Š ç ”ç©¶é¡¹ç›®æ¦‚è§ˆ

æˆ‘ä»¬åˆ†æäº†4ä¸ªé«˜åº¦ç›¸å…³çš„å¼€æºé¡¹ç›®ï¼š

| é¡¹ç›® | Stars | æŠ€æœ¯æ ˆ | æ ¸å¿ƒç‰¹ç‚¹ | é€‚ç”¨æ€§ |
|------|-------|--------|----------|--------|
| **yolo-lstm-fire** | 1 | YOLOv8 + LSTM | ç›´æ¥ç«ç¾æ£€æµ‹ | â­â­â­â­â­ |
| **Fire-Detection** | 76 | YOLOv5 + æ—¶åºè¿½è¸ª | æ—¶ç©ºæ¨¡å¼åˆ†æ | â­â­â­â­ |
| **STCNet** | 37 | åŒæµç½‘ç»œ | æ—¶ç©ºäº¤å‰ç½‘ç»œ | â­â­â­ |
| **YoloV8-LSTM-Violence** | 1 | YOLOv8 + LSTM | æš´åŠ›æ£€æµ‹(å¯è¿ç§») | â­â­â­â­ |

---


## ğŸ”¬ é¡¹ç›®ä¸€ï¼šYOLO-LSTM Fire Detection

### åŸºæœ¬ä¿¡æ¯
- **ä»“åº“**: sureshkumark23/yolo-lstm_fire_detection-in-cctv-videos
- **Stars**: 1
- **è¯­è¨€**: Python
- **æœ€åæ›´æ–°**: 2025-10-11

### æŠ€æœ¯æ¶æ„

```
è§†é¢‘è¾“å…¥ â†’ YOLOv8æ£€æµ‹ â†’ ç‰¹å¾æå– â†’ LSTMåˆ†ç±» â†’ ç«ç¾åˆ¤å®š
```

### æ ¸å¿ƒå®ç°

#### 1. YOLOv8ç›®æ ‡æ£€æµ‹
```python
# åŠ è½½YOLOv8æ¨¡å‹
yolo_model = YOLO("best.pt")

# å¯¹æ¯ä¸€å¸§è¿›è¡Œæ£€æµ‹
results = yolo_model(frame, verbose=False)

# æå–æ£€æµ‹ç»“æœ
for box in results[0].boxes:
    x1, y1, x2, y2 = box.xyxy[0].tolist()  # è¾¹ç•Œæ¡†åæ ‡
    conf = float(box.conf[0])               # ç½®ä¿¡åº¦
    cls = int(box.cls[0])                   # ç±»åˆ«
```

#### 2. ç‰¹å¾æå–ï¼ˆ8ç»´ç‰¹å¾å‘é‡ï¼‰
```python
features = [
    cx,              # ä¸­å¿ƒç‚¹xåæ ‡
    cy,              # ä¸­å¿ƒç‚¹yåæ ‡
    w,               # å®½åº¦
    h,               # é«˜åº¦
    area,            # é¢ç§¯
    aspect_ratio,    # å®½é«˜æ¯”
    conf,            # ç½®ä¿¡åº¦
    cls              # ç±»åˆ«ID
]
```

#### 3. LSTMæ—¶åºåˆ†æ
```python
# ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼ˆ30å¸§ï¼‰
SEQ_LEN = 30
features_buffer = []

# æ”¶é›†30å¸§ç‰¹å¾
if len(features_buffer) >= SEQ_LEN:
    seq = np.array(features_buffer[-SEQ_LEN:])
    seq = np.expand_dims(seq, axis=0)  # (1, 30, 8)
    
    # LSTMé¢„æµ‹
    pred = lstm_model.predict(seq)
    label = np.argmax(pred)  # 0: no_fire, 1: smoke, 2: fire
```

### ä¼˜ç‚¹åˆ†æ

âœ… **ç®€å•ç›´æ¥** - æ¶æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œå®ç°
âœ… **ç«¯åˆ°ç«¯** - ä»è§†é¢‘è¾“å…¥åˆ°ç«ç¾åˆ¤å®šçš„å®Œæ•´æµç¨‹
âœ… **å®æ—¶æ€§å¥½** - ä½¿ç”¨YOLOv8ï¼Œæ¨ç†é€Ÿåº¦å¿«
âœ… **ç‰¹å¾å·¥ç¨‹** - 8ç»´ç‰¹å¾å‘é‡è®¾è®¡åˆç†

### ç¼ºç‚¹åˆ†æ

âŒ **ç‰¹å¾å•ä¸€** - ä»…ä½¿ç”¨è¾¹ç•Œæ¡†å‡ ä½•ç‰¹å¾ï¼Œç¼ºå°‘é¢œè‰²ã€çº¹ç†ç­‰
âŒ **çª—å£å›ºå®š** - 30å¸§å›ºå®šçª—å£å¯èƒ½ä¸é€‚åº”æ‰€æœ‰åœºæ™¯
âŒ **ç¼ºå°‘åå¤„ç†** - æ²¡æœ‰æ—¶åºå¹³æ»‘å’Œè¯¯æŠ¥æŠ‘åˆ¶æœºåˆ¶
âŒ **æ•°æ®é›†æœªå…¬å¼€** - æ— æ³•ç›´æ¥å¤ç°è®­ç»ƒè¿‡ç¨‹

### é€‚ç”¨æ€§è¯„ä¼°

**å¯¹EmberGuard AIçš„ä»·å€¼**: â­â­â­â­â­

è¿™æ˜¯æœ€ç›´æ¥å¯ç”¨çš„æ–¹æ¡ˆï¼Œå¯ä»¥ä½œä¸ºæˆ‘ä»¬çš„**åŸºç¡€æ¶æ„**ï¼š
- ç›´æ¥ä½¿ç”¨YOLOv8 + LSTMçš„ç»„åˆ
- ç‰¹å¾æå–æ–¹æ³•å¯ä»¥ç›´æ¥å€Ÿé‰´
- éœ€è¦æ‰©å±•ç‰¹å¾ç»´åº¦ï¼ˆåŠ å…¥é¢œè‰²ã€è¿åŠ¨ç­‰ï¼‰

---


## ğŸ”¬ é¡¹ç›®äºŒï¼šFire-Detection (æ—¶ç©ºæ¨¡å¼åˆ†æ)

### åŸºæœ¬ä¿¡æ¯
- **ä»“åº“**: pedbrgs/Fire-Detection
- **Stars**: 76 â­ (æœ€å—æ¬¢è¿)
- **è¯­è¨€**: Python
- **æœ€åæ›´æ–°**: 2026-01-06
- **è®ºæ–‡æ”¯æŒ**: å‘è¡¨åœ¨Neural Computing and Applications

### æŠ€æœ¯æ¶æ„

```
è§†é¢‘è¾“å…¥ â†’ YOLOv5æ£€æµ‹ â†’ ç›®æ ‡è¿½è¸ª â†’ æ—¶åºåˆ†æ â†’ ç«ç¾ç¡®è®¤
                                    â†“
                            AVT(é¢ç§¯å˜åŒ–) / TPT(æ—¶åºæŒç»­)
```

### æ ¸å¿ƒåˆ›æ–°ï¼šä¸¤é˜¶æ®µæ··åˆç³»ç»Ÿ

#### é˜¶æ®µ1ï¼šç©ºé—´æ£€æµ‹ (YOLOv5)
```python
# YOLOv5æ£€æµ‹ç«ç„°/çƒŸé›¾å€™é€‰åŒºåŸŸ
pred = model(frame)
det = non_max_suppression(pred, conf_thres, iou_thres)

# æå–è¾¹ç•Œæ¡†
for *xyxy, conf, cls in det:
    xmin, ymin, xmax, ymax = xyxy
    coord_objs.append([xmin, ymin, xmax, ymax])
```

#### é˜¶æ®µ2ï¼šæ—¶åºåˆ†æ

**æ–¹æ³•A: AVT (Area Variation Technique) - é¢ç§¯å˜åŒ–æŠ€æœ¯**
- **é€‚ç”¨åœºæ™¯**: å®¤å¤–åœºæ™¯
- **åŸç†**: çœŸå®ç«ç¾çš„æ£€æµ‹åŒºåŸŸä¼šæŒç»­æ‰©å¤§
- **å®ç°**:
```python
class ObjectTracker:
    def bbox_suppression(self, log):
        for (id, areas) in log.areas.items():
            # è®¡ç®—é¢ç§¯å˜åŒ–çš„å˜å¼‚ç³»æ•°
            var = variation(np.array(areas[-window_size:]))
            
            # å¦‚æœé¢ç§¯å˜åŒ–å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯è¯¯æŠ¥
            if var < area_thresh:
                suppress_detection(id)
```

**æ–¹æ³•B: TPT (Temporal Persistence Technique) - æ—¶åºæŒç»­æŠ€æœ¯**
- **é€‚ç”¨åœºæ™¯**: å®¤å†…åœºæ™¯
- **åŸç†**: çœŸå®ç«ç¾ä¼šåœ¨å¤šå¸§ä¸­æŒç»­å‡ºç°
- **å®ç°**:
```python
temporal_buffer = np.zeros((window_size))

# è®°å½•æ£€æµ‹ç»“æœ
temporal_buffer[pos] = True if detected else False

# è®¡ç®—æŒç»­æ€§
persistence = np.sum(temporal_buffer) / window_size

# å¦‚æœæŒç»­æ€§ä½äºé˜ˆå€¼ï¼ŒæŠ‘åˆ¶æ£€æµ‹
if persistence < persistence_thresh:
    suppress_detection()
```

### ç›®æ ‡è¿½è¸ªå®ç°

```python
class ObjectTracker:
    def tracking(self, coord_objs):
        # è®¡ç®—è´¨å¿ƒ
        centroids = self.compute_centroids(coord_objs)
        
        # è®¡ç®—é¢ç§¯
        areas = self.compute_areas(coord_objs)
        
        # ä½¿ç”¨æ¬§æ°è·ç¦»åŒ¹é…
        D = dist.cdist(object_centroids, centroids)
        
        # åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…
        rows = D.min(axis=1).argsort()
        cols = D.argmin(axis=1)[rows]
        
        # æ›´æ–°è¿½è¸ªå¯¹è±¡
        for (row, col) in zip(rows, cols):
            objectID = object_ids[row]
            self.centroids[objectID] = centroids[col]
            self.areas[objectID] = areas[col]
```

### æ€§èƒ½æŒ‡æ ‡

| æ–¹æ³• | æ£€æµ‹ç‡ | è¯¯æŠ¥ç‡ | é¦–å¸§æ£€æµ‹æ—¶é—´ |
|------|--------|--------|--------------|
| YOLOv5 only | é«˜ | é«˜ | å¿« |
| YOLOv5 + AVT | ä¸­ | ä½ | ä¸­ |
| YOLOv5 + TPT | ä¸­ | æä½ | æ…¢ |

### ä¼˜ç‚¹åˆ†æ

âœ… **å­¦æœ¯ä¸¥è°¨** - æœ‰è®ºæ–‡æ”¯æŒï¼Œæ–¹æ³•ç»è¿‡éªŒè¯
âœ… **è¯¯æŠ¥æ§åˆ¶** - æ—¶åºåˆ†ææœ‰æ•ˆé™ä½è¯¯æŠ¥
âœ… **åœºæ™¯é€‚é…** - AVT/TPTåˆ†åˆ«é€‚é…å®¤å†…å¤–åœºæ™¯
âœ… **ç›®æ ‡è¿½è¸ª** - å®Œæ•´çš„è¿½è¸ªç³»ç»Ÿï¼Œå¯ä»¥è·Ÿè¸ªç«æºæ‰©æ•£
âœ… **ä»£ç å®Œæ•´** - åŒ…å«å®Œæ•´çš„è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æµç¨‹

### ç¼ºç‚¹åˆ†æ

âŒ **YOLOv5** - ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬ï¼Œå¯å‡çº§åˆ°YOLOv8
âŒ **æ‰‹å·¥è§„åˆ™** - AVT/TPTæ˜¯åŸºäºè§„åˆ™çš„ï¼Œä¸å¤Ÿæ™ºèƒ½
âŒ **è®¡ç®—å¼€é”€** - ç›®æ ‡è¿½è¸ªå¢åŠ è®¡ç®—å¤æ‚åº¦
âŒ **å‚æ•°æ•æ„Ÿ** - area_threshã€window_sizeéœ€è¦è°ƒä¼˜

### é€‚ç”¨æ€§è¯„ä¼°

**å¯¹EmberGuard AIçš„ä»·å€¼**: â­â­â­â­

è¿™ä¸ªé¡¹ç›®æä¾›äº†**è¯¯æŠ¥æŠ‘åˆ¶**çš„æœ€ä½³å®è·µï¼š
- AVTé¢ç§¯å˜åŒ–åˆ†æå¯ä»¥ç›´æ¥ä½¿ç”¨
- TPTæ—¶åºæŒç»­æ€§å¯ä»¥ä½œä¸ºLSTMçš„è¡¥å……
- ç›®æ ‡è¿½è¸ªç³»ç»Ÿå¯ä»¥ç”¨äºç«æºæ‰©æ•£åˆ†æ
- éœ€è¦å°†è§„åˆ™æ–¹æ³•æ”¹ä¸ºå­¦ä¹ æ–¹æ³•ï¼ˆLSTMï¼‰

---


## ğŸ”¬ é¡¹ç›®ä¸‰ï¼šSTCNet (æ—¶ç©ºäº¤å‰ç½‘ç»œ)

### åŸºæœ¬ä¿¡æ¯
- **ä»“åº“**: Caoyichao/STCNet
- **Stars**: 37
- **è¯­è¨€**: Python
- **è®ºæ–‡**: arXiv:2011.04863
- **åº”ç”¨**: å·¥ä¸šçƒŸé›¾æ£€æµ‹

### æŠ€æœ¯æ¶æ„ï¼šåŒæµç½‘ç»œ

```
è§†é¢‘è¾“å…¥
    â”œâ”€â†’ RGBå¸§ â”€â”€â”€â”€â†’ ç©ºé—´åˆ†æ”¯ (MobileNetV2/SE-ResNeXt) â”€â”
    â”‚                                                  â”œâ”€â†’ ç‰¹å¾èåˆ â†’ åˆ†ç±»
    â””â”€â†’ å·®åˆ†å¸§ â”€â”€â”€â†’ æ—¶åºåˆ†æ”¯ (MobileNetV2/SE-ResNeXt) â”€â”˜
```

### æ ¸å¿ƒåˆ›æ–°

#### 1. åŒæµè¾“å…¥
```python
# RGBæµ - æ•æ‰ç©ºé—´ç‰¹å¾
rgb_frames = video[t:t+seq_len]

# å·®åˆ†æµ - æ•æ‰è¿åŠ¨ç‰¹å¾
diff_frames = []
for i in range(1, len(rgb_frames)):
    diff = rgb_frames[i] - rgb_frames[i-1]
    diff_frames.append(diff)
```

#### 2. æ—¶ç©ºäº¤å‰æ³¨æ„åŠ›
```python
class STCNet(nn.Module):
    def __init__(self):
        # ç©ºé—´åˆ†æ”¯
        self.spatial_branch = MobileNetV2()
        
        # æ—¶åºåˆ†æ”¯
        self.temporal_branch = MobileNetV2()
        
        # äº¤å‰æ³¨æ„åŠ›æ¨¡å—
        self.cross_attention = CrossAttention()
    
    def forward(self, rgb, diff):
        # æå–ç©ºé—´ç‰¹å¾
        spatial_feat = self.spatial_branch(rgb)
        
        # æå–æ—¶åºç‰¹å¾
        temporal_feat = self.temporal_branch(diff)
        
        # äº¤å‰æ³¨æ„åŠ›èåˆ
        fused_feat = self.cross_attention(spatial_feat, temporal_feat)
        
        return self.classifier(fused_feat)
```

### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | FLOPs | å»¶è¿Ÿ | ååé‡ | F-Score |
|------|--------|-------|------|--------|---------|
| RGB-I3D | 12.3M | 62.7G | 30.56ms | 32.71 vid/s | 0.817 |
| STCNet-MobileNetV2 | **3.7M** | **2.4G** | **9.12ms** | **109.7 vid/s** | **0.868** |
| STCNet-SE-ResNeXt | 27.2M | 34.6G | 23.49ms | 42.57 vid/s | **0.885** |

### ä¼˜ç‚¹åˆ†æ

âœ… **é«˜æ•ˆè½»é‡** - MobileNetV2ç‰ˆæœ¬ä»…3.7Må‚æ•°
âœ… **æ€§èƒ½ä¼˜å¼‚** - F-Scoreè¾¾åˆ°0.868ï¼Œè¶…è¶ŠI3D
âœ… **å®æ—¶æ€§å¼º** - 109.7 vid/sååé‡ï¼Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²
âœ… **åŒæµè®¾è®¡** - åŒæ—¶æ•æ‰ç©ºé—´å’Œæ—¶åºç‰¹å¾
âœ… **å¯è§†åŒ–å¥½** - æä¾›Grad-CAMå¯è§†åŒ–

### ç¼ºç‚¹åˆ†æ

âŒ **éæ£€æµ‹æ¨¡å‹** - æ˜¯åˆ†ç±»æ¨¡å‹ï¼Œä¸èƒ½å®šä½ç«æº
âŒ **å›ºå®šè¾“å…¥** - éœ€è¦å›ºå®šé•¿åº¦çš„è§†é¢‘ç‰‡æ®µ
âŒ **å·¥ä¸šåœºæ™¯** - é’ˆå¯¹å·¥ä¸šçƒŸé›¾ï¼Œéœ€è¦è¿ç§»åˆ°ç«ç¾åœºæ™¯
âŒ **æ— LSTM** - ä½¿ç”¨CNNå¤„ç†æ—¶åºï¼Œä¸å¦‚LSTMçµæ´»

### é€‚ç”¨æ€§è¯„ä¼°

**å¯¹EmberGuard AIçš„ä»·å€¼**: â­â­â­

è¿™ä¸ªé¡¹ç›®æä¾›äº†**è½»é‡åŒ–éƒ¨ç½²**çš„æ€è·¯ï¼š
- åŒæµç½‘ç»œæ¶æ„å¯ä»¥å€Ÿé‰´
- å·®åˆ†å¸§æå–è¿åŠ¨ç‰¹å¾çš„æ–¹æ³•å¾ˆæœ‰ä»·å€¼
- MobileNetV2å¯ä»¥ä½œä¸ºç‰¹å¾æå–backbone
- éœ€è¦æ”¹é€ ä¸ºæ£€æµ‹æ¨¡å‹ï¼ˆåŠ å…¥YOLOï¼‰

---


## ğŸ”¬ é¡¹ç›®å››ï¼šYoloV8-LSTM Violence Detection

### åŸºæœ¬ä¿¡æ¯
- **ä»“åº“**: harmeshgv/YoloV8-LSTM-video-Classification
- **Stars**: 1
- **è¯­è¨€**: Python
- **åº”ç”¨**: æš´åŠ›è¡Œä¸ºæ£€æµ‹
- **ç‰¹ç‚¹**: å®Œæ•´çš„å·¥ç¨‹åŒ–å®ç°

### æŠ€æœ¯æ¶æ„

```
è§†é¢‘ä¸Šä¼  â†’ å¸§æå– â†’ YOLOv8æ£€æµ‹ â†’ ç‰¹å¾æå– â†’ LSTMåˆ†ç±» â†’ æŠ¥å‘Šç”Ÿæˆ
                                                        â†“
                                            FastAPI + Streamlit + React
```

### æ ¸å¿ƒå®ç°

#### 1. è§†é¢‘é¢„å¤„ç†
```python
class VideoDataExtractor:
    def extract_video_data(self, video_path):
        cap = cv2.VideoCapture(video_path)
        frames_data = []
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # YOLOv8æ£€æµ‹
            results = self.yolo_model(frame)
            
            # æå–ç‰¹å¾
            features = self.extract_features(results)
            frames_data.append(features)
        
        return pd.DataFrame(frames_data)
```

#### 2. ç‰¹å¾æå–
```python
class FeatureExtractor:
    def extract_features(self, yolo_results):
        features = {
            'num_persons': 0,
            'avg_confidence': 0,
            'bbox_areas': [],
            'interactions': [],
            'scene_context': []
        }
        
        for detection in yolo_results:
            if detection.cls == 'person':
                features['num_persons'] += 1
                features['bbox_areas'].append(detection.area)
                features['avg_confidence'] += detection.conf
        
        return features
```

#### 3. LSTMåˆ†ç±»å™¨
```python
class ViolencePredictor:
    def __init__(self):
        self.lstm_model = self.build_lstm()
    
    def build_lstm(self):
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(seq_len, features)),
            Dropout(0.3),
            LSTM(64, return_sequences=False),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dense(2, activation='softmax')  # violent / non-violent
        ])
        return model
    
    def predict(self, video_features):
        # æ»‘åŠ¨çª—å£é¢„æµ‹
        predictions = []
        for i in range(0, len(video_features) - seq_len):
            window = video_features[i:i+seq_len]
            pred = self.lstm_model.predict(window)
            predictions.append(pred)
        
        return self.aggregate_predictions(predictions)
```

#### 4. å®Œæ•´çš„Webåº”ç”¨

**FastAPIåç«¯**:
```python
@app.post("/analyze")
async def analyze_video(file: UploadFile):
    # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
    temp_path = save_upload(file)
    
    # æå–ç‰¹å¾
    features = extractor.extract_video_data(temp_path)
    
    # LSTMé¢„æµ‹
    prediction = predictor.predict(features)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(prediction)
    
    return JSONResponse(report)
```

**Streamlitç•Œé¢**:
```python
st.title("Violence Detection System")

uploaded_file = st.file_uploader("Upload Video", type=['mp4', 'avi'])

if uploaded_file:
    with st.spinner("Analyzing..."):
        result = analyze_video(uploaded_file)
    
    st.success(f"Analysis Complete!")
    st.json(result)
```

**Reactå‰ç«¯**:
```typescript
const AnalysisPage = () => {
  const [file, setFile] = useState<File | null>(null);
  const [result, setResult] = useState(null);
  
  const handleAnalyze = async () => {
    const formData = new FormData();
    formData.append('file', file);
    
    const response = await axios.post('/analyze', formData);
    setResult(response.data);
  };
  
  return (
    <div>
      <input type="file" onChange={(e) => setFile(e.target.files[0])} />
      <button onClick={handleAnalyze}>Analyze</button>
      {result && <ResultDisplay data={result} />}
    </div>
  );
};
```

### ä¼˜ç‚¹åˆ†æ

âœ… **å·¥ç¨‹å®Œæ•´** - åŒ…å«å‰åç«¯å®Œæ•´å®ç°
âœ… **ç”¨æˆ·å‹å¥½** - æä¾›Webç•Œé¢å’ŒAPI
âœ… **æ¨¡å—åŒ–å¥½** - ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºæ‰©å±•
âœ… **Dockeræ”¯æŒ** - å®¹å™¨åŒ–éƒ¨ç½²
âœ… **æ–‡æ¡£è¯¦ç»†** - READMEå®Œå–„ï¼Œæ˜“äºä¸Šæ‰‹
âœ… **å®æ—¶åé¦ˆ** - æä¾›è¿›åº¦æ¡å’Œå®æ—¶ç»“æœ

### ç¼ºç‚¹åˆ†æ

âŒ **åœºæ™¯ä¸åŒ** - æš´åŠ›æ£€æµ‹vsç«ç¾æ£€æµ‹
âŒ **ç‰¹å¾ç®€å•** - ä¸»è¦åŸºäºäººå‘˜æ£€æµ‹
âŒ **æ¨¡å‹æœªå¼€æº** - è®­ç»ƒå¥½çš„æ¨¡å‹æœªæä¾›

### é€‚ç”¨æ€§è¯„ä¼°

**å¯¹EmberGuard AIçš„ä»·å€¼**: â­â­â­â­

è¿™ä¸ªé¡¹ç›®æä¾›äº†**å®Œæ•´çš„å·¥ç¨‹åŒ–æ–¹æ¡ˆ**ï¼š
- FastAPI + Streamlit + Reactçš„æ¶æ„å¯ä»¥ç›´æ¥å¤ç”¨
- ç‰¹å¾æå–å’ŒLSTMé¢„æµ‹çš„æµç¨‹å¯ä»¥å€Ÿé‰´
- Dockeréƒ¨ç½²æ–¹æ¡ˆå¯ä»¥ç›´æ¥ä½¿ç”¨
- éœ€è¦å°†äººå‘˜æ£€æµ‹æ”¹ä¸ºç«ç„°/çƒŸé›¾æ£€æµ‹

---


## ğŸ’¡ ç»¼åˆåˆ†æä¸æŠ€æœ¯é€‰å‹

### å„é¡¹ç›®å¯¹æ¯”çŸ©é˜µ

| ç»´åº¦ | YOLO-LSTM-Fire | Fire-Detection | STCNet | YoloV8-LSTM-Violence |
|------|----------------|----------------|--------|----------------------|
| **æ¶æ„ç®€æ´æ€§** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| **æ£€æµ‹ç²¾åº¦** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **å®æ—¶æ€§** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **è¯¯æŠ¥æ§åˆ¶** | â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **å·¥ç¨‹å®Œæ•´æ€§** | â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **å¯æ‰©å±•æ€§** | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **è¾¹ç¼˜éƒ¨ç½²** | â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­ |

### å…³é”®æŠ€æœ¯æå–

#### 1. ç›®æ ‡æ£€æµ‹å±‚
- **é€‰æ‹©**: YOLOv8 (æ¥è‡ªé¡¹ç›®1å’Œ4)
- **ç†ç”±**: æœ€æ–°ç‰ˆæœ¬ï¼Œé€Ÿåº¦å¿«ï¼Œç²¾åº¦é«˜
- **æ”¹è¿›**: ä½¿ç”¨D-Fireæ•°æ®é›†å¾®è°ƒ

#### 2. ç‰¹å¾æå–å±‚
**åŸºç¡€ç‰¹å¾** (æ¥è‡ªé¡¹ç›®1):
- è¾¹ç•Œæ¡†å‡ ä½•ç‰¹å¾: cx, cy, w, h, area, aspect_ratio
- æ£€æµ‹ç½®ä¿¡åº¦: conf
- ç±»åˆ«ä¿¡æ¯: cls

**æ‰©å±•ç‰¹å¾** (æ¥è‡ªé¡¹ç›®2å’Œ3):
- é¢ç§¯å˜åŒ–ç‡: area_change_rate
- è¿åŠ¨ç‰¹å¾: optical_flow, diff_frames
- é¢œè‰²ç‰¹å¾: rgb_histogram, hsv_features
- çº¹ç†ç‰¹å¾: lbp, gabor

**æœ€ç»ˆç‰¹å¾å‘é‡** (16ç»´):
```python
features = [
    # å‡ ä½•ç‰¹å¾ (6ç»´)
    cx, cy, w, h, area, aspect_ratio,
    
    # æ£€æµ‹ç‰¹å¾ (2ç»´)
    conf, cls,
    
    # æ—¶åºç‰¹å¾ (3ç»´)
    area_change_rate, velocity_x, velocity_y,
    
    # é¢œè‰²ç‰¹å¾ (3ç»´)
    mean_r, mean_g, mean_b,
    
    # çº¹ç†ç‰¹å¾ (2ç»´)
    texture_energy, texture_entropy
]
```

#### 3. æ—¶åºåˆ†æå±‚
**LSTMæ¶æ„** (æ¥è‡ªé¡¹ç›®1å’Œ4):
```python
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(30, 16)),
    Dropout(0.3),
    LSTM(64, return_sequences=True),
    Dropout(0.3),
    LSTM(32, return_sequences=False),
    Dense(16, activation='relu'),
    Dense(3, activation='softmax')  # no_fire, smoke, fire
])
```

#### 4. è¯¯æŠ¥æŠ‘åˆ¶å±‚
**æ–¹æ³•A: é¢ç§¯å˜åŒ–åˆ†æ** (æ¥è‡ªé¡¹ç›®2):
```python
def area_variation_check(areas, window=20, thresh=0.05):
    """çœŸå®ç«ç¾çš„é¢ç§¯ä¼šæŒç»­å¢é•¿"""
    var = variation(areas[-window:])
    return var >= thresh
```

**æ–¹æ³•B: æ—¶åºæŒç»­æ€§æ£€æŸ¥** (æ¥è‡ªé¡¹ç›®2):
```python
def temporal_persistence_check(detections, window=20, thresh=0.5):
    """çœŸå®ç«ç¾ä¼šæŒç»­å‡ºç°"""
    persistence = sum(detections[-window:]) / window
    return persistence >= thresh
```

**æ–¹æ³•C: LSTMç½®ä¿¡åº¦å¹³æ»‘**:
```python
def confidence_smoothing(predictions, window=5):
    """ä½¿ç”¨ç§»åŠ¨å¹³å‡å¹³æ»‘é¢„æµ‹ç»“æœ"""
    smoothed = np.convolve(predictions, np.ones(window)/window, mode='valid')
    return smoothed
```

#### 5. ç›®æ ‡è¿½è¸ªå±‚ (æ¥è‡ªé¡¹ç›®2)
```python
class FireTracker:
    def __init__(self):
        self.trackers = {}
        self.next_id = 0
    
    def update(self, detections):
        # è®¡ç®—è´¨å¿ƒ
        centroids = compute_centroids(detections)
        
        # åŒ¹é…ç°æœ‰è¿½è¸ªå¯¹è±¡
        if len(self.trackers) > 0:
            distances = cdist(self.centroids, centroids)
            matches = hungarian_algorithm(distances)
            
            # æ›´æ–°è¿½è¸ªå¯¹è±¡
            for (tracker_id, det_id) in matches:
                self.trackers[tracker_id].update(detections[det_id])
        
        # æ³¨å†Œæ–°å¯¹è±¡
        for unmatched_det in unmatched_detections:
            self.register(unmatched_det)
```

---


## ğŸ¯ EmberGuard AI æŠ€æœ¯å®ç°æ–¹æ¡ˆ

åŸºäºä»¥ä¸Šç ”ç©¶ï¼Œæˆ‘ä»¬åˆ¶å®šä»¥ä¸‹åˆ†é˜¶æ®µå®ç°æ–¹æ¡ˆï¼š

---

### Phase 1: åŸºç¡€YOLO-LSTMç³»ç»Ÿ (2å‘¨)

#### ç›®æ ‡
å®ç°åŸºç¡€çš„ç«ç¾æ£€æµ‹ç³»ç»Ÿï¼Œè¾¾åˆ°90%ä»¥ä¸Šå‡†ç¡®ç‡

#### æŠ€æœ¯æ ˆ
- YOLOv8 (ç›®æ ‡æ£€æµ‹)
- LSTM (æ—¶åºåˆ†æ)
- 8ç»´åŸºç¡€ç‰¹å¾

#### å®ç°æ­¥éª¤

**Step 1.1: YOLOv8å¾®è°ƒ** (3å¤©)
```python
# ä½¿ç”¨D-Fireæ•°æ®é›†è®­ç»ƒ
from ultralytics import YOLO

model = YOLO('yolov8n.pt')
results = model.train(
    data='configs/yolo_fire.yaml',
    epochs=100,
    imgsz=640,
    batch=16,
    device=0
)
```

**Step 1.2: ç‰¹å¾æå–å™¨** (2å¤©)
```python
class FeatureExtractor:
    def extract(self, detection):
        x1, y1, x2, y2 = detection.xyxy[0]
        w, h = x2 - x1, y2 - y1
        
        return np.array([
            (x1 + x2) / 2,  # cx
            (y1 + y2) / 2,  # cy
            w,              # width
            h,              # height
            w * h,          # area
            w / h if h > 0 else 0,  # aspect_ratio
            detection.conf, # confidence
            detection.cls   # class
        ])
```

**Step 1.3: LSTMè®­ç»ƒ** (5å¤©)
```python
# æ•°æ®å‡†å¤‡
def prepare_sequences(video_features, seq_len=30):
    sequences = []
    labels = []
    
    for i in range(len(video_features) - seq_len):
        seq = video_features[i:i+seq_len]
        label = video_labels[i+seq_len]
        sequences.append(seq)
        labels.append(label)
    
    return np.array(sequences), np.array(labels)

# LSTMæ¨¡å‹
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(30, 8)),
    Dropout(0.3),
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

**Step 1.4: æ¨ç†ç®¡é“** (2å¤©)
```python
class FireDetectionPipeline:
    def __init__(self):
        self.yolo = YOLO('runs/detect/train2/weights/best.pt')
        self.lstm = load_model('models/lstm_fire_model.h5')
        self.feature_buffer = []
        self.seq_len = 30
    
    def process_frame(self, frame):
        # YOLOæ£€æµ‹
        results = self.yolo(frame)
        
        # æå–ç‰¹å¾
        features = []
        for det in results[0].boxes:
            feat = self.extract_features(det)
            features.append(feat)
        
        # å¦‚æœæœ‰æ£€æµ‹ç»“æœï¼Œå–ç½®ä¿¡åº¦æœ€é«˜çš„
        if features:
            best_feat = max(features, key=lambda x: x[6])
            self.feature_buffer.append(best_feat)
        else:
            # æ— æ£€æµ‹ï¼Œå¡«å……é›¶å‘é‡
            self.feature_buffer.append(np.zeros(8))
        
        # LSTMé¢„æµ‹
        if len(self.feature_buffer) >= self.seq_len:
            seq = np.array(self.feature_buffer[-self.seq_len:])
            seq = np.expand_dims(seq, axis=0)
            pred = self.lstm.predict(seq, verbose=0)
            return pred
        
        return None
```

#### é¢„æœŸæˆæœ
- âœ… åŸºç¡€ç«ç¾æ£€æµ‹åŠŸèƒ½
- âœ… å®æ—¶æ¨ç†èƒ½åŠ› (~30 FPS)
- âœ… å‡†ç¡®ç‡ > 90%
- âœ… å¯è§†åŒ–æ£€æµ‹ç»“æœ

---

### Phase 2: è¯¯æŠ¥æŠ‘åˆ¶ä¸ä¼˜åŒ– (2å‘¨)

#### ç›®æ ‡
é™ä½è¯¯æŠ¥ç‡è‡³ < 5%ï¼Œæå‡ç³»ç»Ÿé²æ£’æ€§

#### æ–°å¢åŠŸèƒ½

**2.1: æ‰©å±•ç‰¹å¾ç»´åº¦** (3å¤©)
```python
class EnhancedFeatureExtractor:
    def extract(self, detection, frame, prev_frame):
        # åŸºç¡€ç‰¹å¾ (8ç»´)
        basic_feat = self.extract_basic(detection)
        
        # é¢œè‰²ç‰¹å¾ (3ç»´)
        roi = frame[y1:y2, x1:x2]
        color_feat = np.mean(roi, axis=(0, 1))  # RGBå‡å€¼
        
        # è¿åŠ¨ç‰¹å¾ (2ç»´)
        if prev_frame is not None:
            flow = cv2.calcOpticalFlowFarneback(
                cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY),
                cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
                None, 0.5, 3, 15, 3, 5, 1.2, 0
            )
            motion_feat = [
                np.mean(flow[..., 0]),  # xæ–¹å‘è¿åŠ¨
                np.mean(flow[..., 1])   # yæ–¹å‘è¿åŠ¨
            ]
        else:
            motion_feat = [0, 0]
        
        # çº¹ç†ç‰¹å¾ (3ç»´)
        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        texture_feat = [
            np.std(gray_roi),           # æ ‡å‡†å·®
            cv2.Laplacian(gray_roi, cv2.CV_64F).var(),  # æ‹‰æ™®æ‹‰æ–¯æ–¹å·®
            np.mean(np.abs(np.diff(gray_roi, axis=0)))  # æ¢¯åº¦
        ]
        
        # åˆå¹¶ç‰¹å¾ (16ç»´)
        return np.concatenate([
            basic_feat,    # 8ç»´
            color_feat,    # 3ç»´
            motion_feat,   # 2ç»´
            texture_feat   # 3ç»´
        ])
```

**2.2: ç›®æ ‡è¿½è¸ªç³»ç»Ÿ** (4å¤©)
```python
from scipy.spatial import distance as dist

class FireObjectTracker:
    def __init__(self, max_disappeared=30):
        self.objects = OrderedDict()
        self.disappeared = OrderedDict()
        self.areas = OrderedDict()
        self.next_id = 0
        self.max_disappeared = max_disappeared
    
    def register(self, centroid, area):
        self.objects[self.next_id] = centroid
        self.areas[self.next_id] = [area]
        self.disappeared[self.next_id] = 0
        self.next_id += 1
    
    def deregister(self, object_id):
        del self.objects[object_id]
        del self.disappeared[object_id]
        del self.areas[object_id]
    
    def update(self, detections):
        # å¦‚æœæ²¡æœ‰æ£€æµ‹ç»“æœ
        if len(detections) == 0:
            for object_id in list(self.disappeared.keys()):
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return self.objects
        
        # è®¡ç®—è´¨å¿ƒå’Œé¢ç§¯
        centroids = []
        areas = []
        for det in detections:
            cx = (det[0] + det[2]) / 2
            cy = (det[1] + det[3]) / 2
            area = (det[2] - det[0]) * (det[3] - det[1])
            centroids.append((cx, cy))
            areas.append(area)
        
        # å¦‚æœæ²¡æœ‰è¿½è¸ªå¯¹è±¡ï¼Œæ³¨å†Œæ‰€æœ‰æ£€æµ‹
        if len(self.objects) == 0:
            for i in range(len(centroids)):
                self.register(centroids[i], areas[i])
        else:
            # åŒ¹é…ç°æœ‰å¯¹è±¡
            object_ids = list(self.objects.keys())
            object_centroids = list(self.objects.values())
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            D = dist.cdist(np.array(object_centroids), centroids)
            
            # åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…
            rows = D.min(axis=1).argsort()
            cols = D.argmin(axis=1)[rows]
            
            used_rows = set()
            used_cols = set()
            
            for (row, col) in zip(rows, cols):
                if row in used_rows or col in used_cols:
                    continue
                
                object_id = object_ids[row]
                self.objects[object_id] = centroids[col]
                self.areas[object_id].append(areas[col])
                self.disappeared[object_id] = 0
                
                used_rows.add(row)
                used_cols.add(col)
            
            # å¤„ç†æœªåŒ¹é…çš„å¯¹è±¡
            unused_rows = set(range(D.shape[0])).difference(used_rows)
            unused_cols = set(range(D.shape[1])).difference(used_cols)
            
            for row in unused_rows:
                object_id = object_ids[row]
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            
            for col in unused_cols:
                self.register(centroids[col], areas[col])
        
        return self.objects
```

**2.3: è¯¯æŠ¥æŠ‘åˆ¶æ¨¡å—** (3å¤©)
```python
from scipy.stats import variation

class FalsePositiveSuppressor:
    def __init__(self, area_thresh=0.05, persistence_thresh=0.5, window_size=20):
        self.area_thresh = area_thresh
        self.persistence_thresh = persistence_thresh
        self.window_size = window_size
    
    def check_area_variation(self, tracker, object_id):
        """æ£€æŸ¥é¢ç§¯å˜åŒ– - çœŸå®ç«ç¾ä¼šæ‰©å¤§"""
        areas = tracker.areas[object_id]
        if len(areas) < self.window_size:
            return True  # æ•°æ®ä¸è¶³ï¼Œæš‚ä¸æŠ‘åˆ¶
        
        recent_areas = areas[-self.window_size:]
        var = variation(recent_areas)
        
        return var >= self.area_thresh
    
    def check_temporal_persistence(self, detection_history):
        """æ£€æŸ¥æ—¶åºæŒç»­æ€§ - çœŸå®ç«ç¾ä¼šæŒç»­å‡ºç°"""
        if len(detection_history) < self.window_size:
            return True
        
        recent = detection_history[-self.window_size:]
        persistence = sum(recent) / len(recent)
        
        return persistence >= self.persistence_thresh
    
    def check_color_consistency(self, color_history):
        """æ£€æŸ¥é¢œè‰²ä¸€è‡´æ€§ - ç«ç„°é¢œè‰²åº”è¯¥ç¨³å®š"""
        if len(color_history) < 10:
            return True
        
        recent_colors = np.array(color_history[-10:])
        color_std = np.std(recent_colors, axis=0)
        
        # ç«ç„°é¢œè‰²åº”è¯¥åœ¨çº¢-æ©™-é»„èŒƒå›´å†…ä¸”ç›¸å¯¹ç¨³å®š
        return np.mean(color_std) < 30  # é˜ˆå€¼å¯è°ƒ
    
    def should_suppress(self, tracker, object_id, detection_history, color_history):
        """ç»¼åˆåˆ¤æ–­æ˜¯å¦åº”è¯¥æŠ‘åˆ¶æ£€æµ‹"""
        checks = [
            self.check_area_variation(tracker, object_id),
            self.check_temporal_persistence(detection_history),
            self.check_color_consistency(color_history)
        ]
        
        # è‡³å°‘é€šè¿‡2/3çš„æ£€æŸ¥æ‰ä¸æŠ‘åˆ¶
        return sum(checks) < 2
```

**2.4: é›†æˆç®¡é“** (4å¤©)
```python
class EnhancedFireDetectionPipeline:
    def __init__(self):
        self.yolo = YOLO('runs/detect/train2/weights/best.pt')
        self.lstm = load_model('models/lstm_fire_model_v2.h5')
        self.feature_extractor = EnhancedFeatureExtractor()
        self.tracker = FireObjectTracker()
        self.suppressor = FalsePositiveSuppressor()
        
        self.feature_buffer = []
        self.detection_history = []
        self.color_history = defaultdict(list)
        self.prev_frame = None
        self.seq_len = 30
    
    def process_frame(self, frame):
        # YOLOæ£€æµ‹
        results = self.yolo(frame)
        
        # æ›´æ–°è¿½è¸ªå™¨
        detections = []
        for det in results[0].boxes:
            bbox = det.xyxy[0].cpu().numpy()
            detections.append(bbox)
        
        tracked_objects = self.tracker.update(detections)
        
        # æå–ç‰¹å¾
        features = []
        for det in results[0].boxes:
            feat = self.feature_extractor.extract(det, frame, self.prev_frame)
            features.append(feat)
            
            # è®°å½•é¢œè‰²å†å²
            object_id = self.find_object_id(det, tracked_objects)
            if object_id is not None:
                self.color_history[object_id].append(feat[8:11])
        
        # è¯¯æŠ¥æŠ‘åˆ¶
        valid_features = []
        for i, feat in enumerate(features):
            object_id = self.find_object_id(results[0].boxes[i], tracked_objects)
            if object_id is not None:
                if not self.suppressor.should_suppress(
                    self.tracker, object_id, 
                    self.detection_history, 
                    self.color_history[object_id]
                ):
                    valid_features.append(feat)
        
        # æ›´æ–°æ£€æµ‹å†å²
        self.detection_history.append(len(valid_features) > 0)
        
        # LSTMé¢„æµ‹
        if valid_features:
            best_feat = max(valid_features, key=lambda x: x[6])
            self.feature_buffer.append(best_feat)
        else:
            self.feature_buffer.append(np.zeros(16))
        
        if len(self.feature_buffer) >= self.seq_len:
            seq = np.array(self.feature_buffer[-self.seq_len:])
            seq = np.expand_dims(seq, axis=0)
            pred = self.lstm.predict(seq, verbose=0)
            
            self.prev_frame = frame.copy()
            return pred, tracked_objects
        
        self.prev_frame = frame.copy()
        return None, tracked_objects
```

#### é¢„æœŸæˆæœ
- âœ… è¯¯æŠ¥ç‡ < 5%
- âœ… å‡†ç¡®ç‡ > 95%
- âœ… ç›®æ ‡è¿½è¸ªåŠŸèƒ½
- âœ… ç«æºæ‰©æ•£åˆ†æ

---


### Phase 3: ç‚ŠçƒŸvsç«ç¾çƒŸé›¾åŒºåˆ† (2å‘¨)

#### ç›®æ ‡
å®ç°è¯´æ˜ä¹¦ä¸­çš„"åŒºåˆ†ç‚ŠçƒŸä¸ç«ç¾çƒŸé›¾"åŠŸèƒ½ï¼Œè¯¯æŠ¥ç‡é™è‡³ < 2%

#### æ ¸å¿ƒæŒ‘æˆ˜
ç‚ŠçƒŸå’Œç«ç¾çƒŸé›¾çš„è§†è§‰ç‰¹å¾ç›¸ä¼¼ï¼Œéœ€è¦ä»ä»¥ä¸‹ç»´åº¦åŒºåˆ†ï¼š
1. **è¿åŠ¨æ¨¡å¼**: ç‚ŠçƒŸä¸Šå‡å¹³ç¨³ï¼Œç«ç¾çƒŸé›¾æ‰©æ•£å¿«é€Ÿ
2. **é¢œè‰²å˜åŒ–**: ç‚ŠçƒŸé¢œè‰²å•ä¸€ï¼Œç«ç¾çƒŸé›¾é¢œè‰²å¤šå˜
3. **æŒç»­æ—¶é—´**: ç‚ŠçƒŸçŸ­æš‚ï¼Œç«ç¾çƒŸé›¾æŒç»­
4. **ä¼´éšç‰¹å¾**: ç«ç¾çƒŸé›¾å¸¸ä¼´éšç«ç„°

#### å®ç°æ–¹æ¡ˆ

**3.1: çƒŸé›¾ç‰¹å¾æå–å™¨** (4å¤©)
```python
class SmokeFeatureExtractor:
    def __init__(self):
        self.optical_flow = cv2.FarnebackOpticalFlow_create()
    
    def extract_smoke_features(self, smoke_roi, frame_sequence):
        """æå–çƒŸé›¾ä¸“ç”¨ç‰¹å¾"""
        features = {}
        
        # 1. è¿åŠ¨ç‰¹å¾
        features['motion'] = self.analyze_motion_pattern(frame_sequence)
        
        # 2. æ‰©æ•£é€Ÿåº¦
        features['expansion_rate'] = self.calculate_expansion_rate(frame_sequence)
        
        # 3. é¢œè‰²æ—¶åºå˜åŒ–
        features['color_variance'] = self.analyze_color_variance(frame_sequence)
        
        # 4. çº¹ç†å¤æ‚åº¦
        features['texture'] = self.analyze_texture(smoke_roi)
        
        # 5. å½¢çŠ¶å˜åŒ–
        features['shape_change'] = self.analyze_shape_change(frame_sequence)
        
        return features
    
    def analyze_motion_pattern(self, frame_sequence):
        """åˆ†æè¿åŠ¨æ¨¡å¼"""
        flows = []
        for i in range(1, len(frame_sequence)):
            prev = cv2.cvtColor(frame_sequence[i-1], cv2.COLOR_BGR2GRAY)
            curr = cv2.cvtColor(frame_sequence[i], cv2.COLOR_BGR2GRAY)
            flow = cv2.calcOpticalFlowFarneback(prev, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)
            flows.append(flow)
        
        # è®¡ç®—è¿åŠ¨æ–¹å‘çš„ä¸€è‡´æ€§
        flow_directions = [np.arctan2(f[..., 1], f[..., 0]) for f in flows]
        direction_std = np.std(flow_directions)
        
        # è®¡ç®—è¿åŠ¨é€Ÿåº¦
        flow_magnitudes = [np.sqrt(f[..., 0]**2 + f[..., 1]**2) for f in flows]
        avg_speed = np.mean(flow_magnitudes)
        
        return {
            'direction_consistency': 1 / (1 + direction_std),  # ç‚ŠçƒŸæ–¹å‘ä¸€è‡´
            'average_speed': avg_speed,
            'speed_variance': np.std(flow_magnitudes)  # ç«ç¾çƒŸé›¾é€Ÿåº¦å˜åŒ–å¤§
        }
    
    def calculate_expansion_rate(self, frame_sequence):
        """è®¡ç®—æ‰©æ•£é€Ÿåº¦"""
        areas = []
        for frame in frame_sequence:
            # å‡è®¾å·²ç»åˆ†å‰²å‡ºçƒŸé›¾åŒºåŸŸ
            smoke_mask = self.segment_smoke(frame)
            area = np.sum(smoke_mask > 0)
            areas.append(area)
        
        # è®¡ç®—é¢ç§¯å¢é•¿ç‡
        if len(areas) > 1:
            growth_rates = np.diff(areas) / areas[:-1]
            return {
                'avg_growth_rate': np.mean(growth_rates),
                'max_growth_rate': np.max(growth_rates),
                'growth_acceleration': np.std(growth_rates)  # ç«ç¾åŠ é€Ÿæ‰©æ•£
            }
        return {'avg_growth_rate': 0, 'max_growth_rate': 0, 'growth_acceleration': 0}
    
    def analyze_color_variance(self, frame_sequence):
        """åˆ†æé¢œè‰²æ—¶åºå˜åŒ–"""
        colors = []
        for frame in frame_sequence:
            smoke_roi = self.extract_smoke_roi(frame)
            mean_color = np.mean(smoke_roi, axis=(0, 1))
            colors.append(mean_color)
        
        colors = np.array(colors)
        
        return {
            'color_std': np.mean(np.std(colors, axis=0)),  # ç«ç¾çƒŸé›¾é¢œè‰²å˜åŒ–å¤§
            'color_trend': self.calculate_color_trend(colors),  # ç«ç¾çƒŸé›¾é¢œè‰²å˜æ·±
            'color_range': np.ptp(colors, axis=0).mean()  # é¢œè‰²èŒƒå›´
        }
    
    def analyze_texture(self, smoke_roi):
        """åˆ†æçº¹ç†å¤æ‚åº¦"""
        gray = cv2.cvtColor(smoke_roi, cv2.COLOR_BGR2GRAY)
        
        # LBPçº¹ç†
        lbp = self.calculate_lbp(gray)
        
        # Gaboræ»¤æ³¢
        gabor_features = self.calculate_gabor(gray)
        
        # è¾¹ç¼˜å¯†åº¦
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        return {
            'lbp_variance': np.var(lbp),
            'gabor_energy': np.mean(gabor_features),
            'edge_density': edge_density  # ç«ç¾çƒŸé›¾è¾¹ç¼˜æ›´å¤æ‚
        }
```

**3.2: çƒŸé›¾åˆ†ç±»å™¨** (3å¤©)
```python
class SmokeCookingFireClassifier:
    """ä¸‰åˆ†ç±»å™¨: ç‚ŠçƒŸ / ç«ç¾çƒŸé›¾ / æ— çƒŸé›¾"""
    
    def __init__(self):
        self.model = self.build_model()
    
    def build_model(self):
        """æ„å»ºä¸“é—¨çš„çƒŸé›¾åˆ†ç±»æ¨¡å‹"""
        model = Sequential([
            # è¾“å…¥: çƒŸé›¾ç‰¹å¾åºåˆ— (seq_len, feature_dim)
            LSTM(64, return_sequences=True, input_shape=(20, 32)),
            Dropout(0.3),
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            Dense(16, activation='relu'),
            Dense(3, activation='softmax')  # cooking_smoke, fire_smoke, no_smoke
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_features(self, smoke_features_sequence):
        """å‡†å¤‡è¾“å…¥ç‰¹å¾"""
        feature_vector = []
        
        for features in smoke_features_sequence:
            vec = np.concatenate([
                # è¿åŠ¨ç‰¹å¾ (3ç»´)
                [features['motion']['direction_consistency'],
                 features['motion']['average_speed'],
                 features['motion']['speed_variance']],
                
                # æ‰©æ•£ç‰¹å¾ (3ç»´)
                [features['expansion_rate']['avg_growth_rate'],
                 features['expansion_rate']['max_growth_rate'],
                 features['expansion_rate']['growth_acceleration']],
                
                # é¢œè‰²ç‰¹å¾ (3ç»´)
                [features['color_variance']['color_std'],
                 features['color_variance']['color_trend'],
                 features['color_variance']['color_range']],
                
                # çº¹ç†ç‰¹å¾ (3ç»´)
                [features['texture']['lbp_variance'],
                 features['texture']['gabor_energy'],
                 features['texture']['edge_density']],
                
                # ... å…¶ä»–ç‰¹å¾
            ])
            
            feature_vector.append(vec)
        
        return np.array(feature_vector)
    
    def classify(self, smoke_features_sequence):
        """åˆ†ç±»çƒŸé›¾ç±»å‹"""
        features = self.prepare_features(smoke_features_sequence)
        features = np.expand_dims(features, axis=0)
        
        pred = self.model.predict(features)
        
        classes = ['cooking_smoke', 'fire_smoke', 'no_smoke']
        class_idx = np.argmax(pred)
        confidence = pred[0][class_idx]
        
        return {
            'class': classes[class_idx],
            'confidence': float(confidence),
            'probabilities': {
                'cooking_smoke': float(pred[0][0]),
                'fire_smoke': float(pred[0][1]),
                'no_smoke': float(pred[0][2])
            }
        }
```

**3.3: é›†æˆåˆ°ä¸»ç®¡é“** (3å¤©)
```python
class FireDetectionWithSmokeClassification:
    def __init__(self):
        self.fire_detector = EnhancedFireDetectionPipeline()
        self.smoke_extractor = SmokeFeatureExtractor()
        self.smoke_classifier = SmokeCookingFireClassifier()
        
        self.smoke_feature_buffer = []
        self.smoke_seq_len = 20
    
    def process_frame(self, frame):
        # 1. åŸºç¡€ç«ç¾æ£€æµ‹
        fire_pred, tracked_objects = self.fire_detector.process_frame(frame)
        
        if fire_pred is None:
            return None
        
        # 2. å¦‚æœæ£€æµ‹åˆ°çƒŸé›¾ç±»åˆ«
        fire_class = np.argmax(fire_pred)
        if fire_class == 1:  # smoke class
            # æå–çƒŸé›¾ç‰¹å¾
            smoke_features = self.smoke_extractor.extract_smoke_features(
                smoke_roi=self.extract_smoke_roi(frame, tracked_objects),
                frame_sequence=self.get_recent_frames()
            )
            
            self.smoke_feature_buffer.append(smoke_features)
            
            # çƒŸé›¾åˆ†ç±»
            if len(self.smoke_feature_buffer) >= self.smoke_seq_len:
                smoke_classification = self.smoke_classifier.classify(
                    self.smoke_feature_buffer[-self.smoke_seq_len:]
                )
                
                # å¦‚æœæ˜¯ç‚ŠçƒŸï¼ŒæŠ‘åˆ¶å‘Šè­¦
                if smoke_classification['class'] == 'cooking_smoke':
                    if smoke_classification['confidence'] > 0.8:
                        return {
                            'alert': False,
                            'reason': 'cooking_smoke_detected',
                            'confidence': smoke_classification['confidence']
                        }
                
                # å¦‚æœæ˜¯ç«ç¾çƒŸé›¾ï¼Œå¢å¼ºå‘Šè­¦
                elif smoke_classification['class'] == 'fire_smoke':
                    return {
                        'alert': True,
                        'type': 'fire_smoke',
                        'confidence': smoke_classification['confidence'],
                        'fire_prediction': fire_pred,
                        'tracked_objects': tracked_objects
                    }
        
        # 3. å¦‚æœæ£€æµ‹åˆ°ç«ç„°
        elif fire_class == 2:  # fire class
            return {
                'alert': True,
                'type': 'fire',
                'confidence': float(fire_pred[0][fire_class]),
                'tracked_objects': tracked_objects
            }
        
        return {
            'alert': False,
            'reason': 'no_fire_detected'
        }
```

**3.4: æ•°æ®å¢å¼ºä¸è®­ç»ƒ** (4å¤©)
```python
# æ•°æ®å¢å¼ºç­–ç•¥
def augment_smoke_data(video_path, label):
    """é’ˆå¯¹çƒŸé›¾æ•°æ®çš„å¢å¼º"""
    augmentations = [
        # äº®åº¦å˜åŒ–
        A.RandomBrightnessContrast(p=0.5),
        
        # æ¨¡ç³Š
        A.GaussianBlur(p=0.3),
        
        # å™ªå£°
        A.GaussNoise(p=0.3),
        
        # é¢œè‰²æŠ–åŠ¨
        A.ColorJitter(p=0.5),
        
        # æ—¶é—´æ‰­æ›² (æ”¹å˜æ’­æ”¾é€Ÿåº¦)
        TimeWarp(rate_range=(0.8, 1.2)),
        
        # å¸§ä¸¢å¤± (æ¨¡æ‹Ÿç½‘ç»œä¸ç¨³å®š)
        FrameDrop(drop_rate=0.1)
    ]
    
    return apply_augmentations(video_path, augmentations)

# è®­ç»ƒç­–ç•¥
def train_smoke_classifier():
    # æ•°æ®é›†åˆ’åˆ†
    # - ç‚ŠçƒŸè§†é¢‘: 1000ä¸ª
    # - ç«ç¾çƒŸé›¾è§†é¢‘: 1000ä¸ª
    # - æ— çƒŸé›¾è§†é¢‘: 500ä¸ª
    
    train_data, val_data, test_data = prepare_smoke_dataset()
    
    # ç±»åˆ«æƒé‡ (å¤„ç†ä¸å¹³è¡¡)
    class_weights = {
        0: 1.0,  # cooking_smoke
        1: 1.5,  # fire_smoke (æ›´é‡è¦)
        2: 0.5   # no_smoke
    }
    
    # è®­ç»ƒ
    model.fit(
        train_data,
        validation_data=val_data,
        epochs=50,
        batch_size=32,
        class_weight=class_weights,
        callbacks=[
            EarlyStopping(patience=10),
            ReduceLROnPlateau(patience=5),
            ModelCheckpoint('best_smoke_classifier.h5')
        ]
    )
```

#### é¢„æœŸæˆæœ
- âœ… ç‚ŠçƒŸè¯†åˆ«å‡†ç¡®ç‡ > 90%
- âœ… ç«ç¾çƒŸé›¾è¯†åˆ«å‡†ç¡®ç‡ > 95%
- âœ… æ€»ä½“è¯¯æŠ¥ç‡ < 2%
- âœ… å®ç°è¯´æ˜ä¹¦æ ¸å¿ƒåŠŸèƒ½

---


### Phase 4: å·¥ç¨‹åŒ–ä¸éƒ¨ç½² (2å‘¨)

#### ç›®æ ‡
å®Œæ•´çš„Webåº”ç”¨ã€APIæ¥å£ã€Dockeréƒ¨ç½²

#### æŠ€æœ¯æ ˆ
- **åç«¯**: FastAPI
- **å‰ç«¯**: Streamlit + React (å¯é€‰)
- **éƒ¨ç½²**: Docker + Docker Compose
- **ç›‘æ§**: Prometheus + Grafana (å¯é€‰)

#### å®ç°æ­¥éª¤

**4.1: FastAPIåç«¯** (3å¤©)
```python
# main.py
from fastapi import FastAPI, UploadFile, File, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import cv2
import numpy as np

app = FastAPI(title="EmberGuard AI API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æ£€æµ‹å™¨
detector = FireDetectionWithSmokeClassification()

@app.post("/api/analyze/video")
async def analyze_video(file: UploadFile = File(...)):
    """åˆ†æä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶"""
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_path = f"temp/{file.filename}"
    with open(temp_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # å¤„ç†è§†é¢‘
    results = []
    cap = cv2.VideoCapture(temp_path)
    frame_count = 0
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        result = detector.process_frame(frame)
        if result and result.get('alert'):
            results.append({
                'frame': frame_count,
                'timestamp': frame_count / cap.get(cv2.CAP_PROP_FPS),
                'type': result['type'],
                'confidence': result['confidence']
            })
        
        frame_count += 1
    
    cap.release()
    
    return {
        'total_frames': frame_count,
        'alerts': results,
        'summary': generate_summary(results)
    }

@app.websocket("/ws/realtime")
async def realtime_detection(websocket: WebSocket):
    """å®æ—¶æ£€æµ‹WebSocketæ¥å£"""
    await websocket.accept()
    
    try:
        while True:
            # æ¥æ”¶å¸§æ•°æ®
            data = await websocket.receive_bytes()
            
            # è§£ç å›¾åƒ
            nparr = np.frombuffer(data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            # æ£€æµ‹
            result = detector.process_frame(frame)
            
            # å‘é€ç»“æœ
            if result:
                await websocket.send_json(result)
    
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        await websocket.close()

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        'status': 'healthy',
        'model_loaded': detector is not None,
        'version': '1.0.0'
    }

@app.get("/api/stats")
async def get_statistics():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    return {
        'total_detections': detector.get_total_detections(),
        'false_positives_suppressed': detector.get_suppressed_count(),
        'average_confidence': detector.get_average_confidence()
    }
```

**4.2: Streamlitç•Œé¢** (2å¤©)
```python
# streamlit_app.py
import streamlit as st
import requests
import cv2
from PIL import Image

st.set_page_config(
    page_title="EmberGuard AI",
    page_icon="ğŸ”¥",
    layout="wide"
)

st.title("ğŸ”¥ EmberGuard AI - æ™ºèƒ½ç«ç¾æ£€æµ‹ç³»ç»Ÿ")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("è®¾ç½®")
    detection_mode = st.selectbox(
        "æ£€æµ‹æ¨¡å¼",
        ["è§†é¢‘æ–‡ä»¶", "å®æ—¶æ‘„åƒå¤´", "RTSPæµ"]
    )
    
    confidence_threshold = st.slider(
        "ç½®ä¿¡åº¦é˜ˆå€¼",
        0.0, 1.0, 0.7, 0.05
    )
    
    enable_smoke_classification = st.checkbox(
        "å¯ç”¨ç‚ŠçƒŸè¯†åˆ«",
        value=True
    )

# ä¸»ç•Œé¢
if detection_mode == "è§†é¢‘æ–‡ä»¶":
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
        type=['mp4', 'avi', 'mov']
    )
    
    if uploaded_file:
        if st.button("å¼€å§‹åˆ†æ"):
            with st.spinner("æ­£åœ¨åˆ†æè§†é¢‘..."):
                # è°ƒç”¨API
                files = {'file': uploaded_file}
                response = requests.post(
                    "http://localhost:8000/api/analyze/video",
                    files=files
                )
                
                if response.status_code == 200:
                    results = response.json()
                    
                    # æ˜¾ç¤ºç»“æœ
                    st.success(f"åˆ†æå®Œæˆï¼å…±æ£€æµ‹åˆ° {len(results['alerts'])} ä¸ªå¼‚å¸¸")
                    
                    # æ—¶é—´çº¿
                    st.subheader("æ£€æµ‹æ—¶é—´çº¿")
                    for alert in results['alerts']:
                        col1, col2, col3 = st.columns([2, 2, 1])
                        with col1:
                            st.write(f"â±ï¸ {alert['timestamp']:.2f}s")
                        with col2:
                            st.write(f"ğŸ”¥ {alert['type']}")
                        with col3:
                            st.write(f"ğŸ“Š {alert['confidence']:.2%}")
                    
                    # ç»Ÿè®¡å›¾è¡¨
                    st.subheader("ç»Ÿè®¡åˆ†æ")
                    import plotly.express as px
                    import pandas as pd
                    
                    df = pd.DataFrame(results['alerts'])
                    fig = px.line(df, x='timestamp', y='confidence', 
                                  title='ç½®ä¿¡åº¦å˜åŒ–æ›²çº¿')
                    st.plotly_chart(fig)

elif detection_mode == "å®æ—¶æ‘„åƒå¤´":
    st.subheader("å®æ—¶æ£€æµ‹")
    
    # æ‘„åƒå¤´é€‰æ‹©
    camera_id = st.number_input("æ‘„åƒå¤´ID", 0, 10, 0)
    
    if st.button("å¼€å§‹æ£€æµ‹"):
        # åˆ›å»ºå ä½ç¬¦
        frame_placeholder = st.empty()
        alert_placeholder = st.empty()
        
        # æ‰“å¼€æ‘„åƒå¤´
        cap = cv2.VideoCapture(camera_id)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ£€æµ‹
            result = detector.process_frame(frame)
            
            # æ˜¾ç¤ºå¸§
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_placeholder.image(frame_rgb, channels="RGB")
            
            # æ˜¾ç¤ºå‘Šè­¦
            if result and result.get('alert'):
                alert_placeholder.error(
                    f"âš ï¸ æ£€æµ‹åˆ°{result['type']}ï¼ç½®ä¿¡åº¦: {result['confidence']:.2%}"
                )
            else:
                alert_placeholder.success("âœ… æ­£å¸¸")
        
        cap.release()
```

**4.3: Dockeréƒ¨ç½²** (3å¤©)
```dockerfile
# Dockerfile
FROM python:3.11-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000 8501

# å¯åŠ¨å‘½ä»¤
CMD ["sh", "-c", "uvicorn main:app --host 0.0.0.0 --port 8000 & streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  emberguard-api:
    build: .
    container_name: emberguard-api
    ports:
      - "8000:8000"
      - "8501:8501"
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
      - ./logs:/app/logs
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # å¯é€‰: Redisç¼“å­˜
  redis:
    image: redis:alpine
    container_name: emberguard-redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # å¯é€‰: PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    container_name: emberguard-db
    environment:
      POSTGRES_DB: emberguard
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  postgres_data:
```

**4.4: æ€§èƒ½ä¼˜åŒ–** (3å¤©)
```python
# æ¨¡å‹é‡åŒ–
def quantize_model(model_path, output_path):
    """é‡åŒ–æ¨¡å‹ä»¥å‡å°ä½“ç§¯å’Œæå‡é€Ÿåº¦"""
    import torch
    
    model = torch.load(model_path)
    model.eval()
    
    # åŠ¨æ€é‡åŒ–
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear, torch.nn.LSTM},
        dtype=torch.qint8
    )
    
    torch.save(quantized_model, output_path)
    
    # å¯¹æ¯”å¤§å°
    original_size = os.path.getsize(model_path) / 1024 / 1024
    quantized_size = os.path.getsize(output_path) / 1024 / 1024
    
    print(f"åŸå§‹æ¨¡å‹: {original_size:.2f} MB")
    print(f"é‡åŒ–æ¨¡å‹: {quantized_size:.2f} MB")
    print(f"å‹ç¼©ç‡: {(1 - quantized_size/original_size)*100:.1f}%")

# æ‰¹å¤„ç†ä¼˜åŒ–
class BatchProcessor:
    def __init__(self, batch_size=8):
        self.batch_size = batch_size
        self.frame_buffer = []
    
    def add_frame(self, frame):
        self.frame_buffer.append(frame)
        
        if len(self.frame_buffer) >= self.batch_size:
            return self.process_batch()
        
        return None
    
    def process_batch(self):
        """æ‰¹é‡å¤„ç†å¸§"""
        batch = np.array(self.frame_buffer)
        
        # YOLOæ‰¹é‡æ¨ç†
        results = self.yolo(batch)
        
        # æ¸…ç©ºç¼“å†²åŒº
        self.frame_buffer = []
        
        return results

# ç¼“å­˜æœºåˆ¶
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_feature_extraction(frame_hash):
    """ç¼“å­˜ç‰¹å¾æå–ç»“æœ"""
    # ç‰¹å¾æå–é€»è¾‘
    pass
```

**4.5: ç›‘æ§ä¸æ—¥å¿—** (2å¤©)
```python
# logging_config.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logging():
    logger = logging.getLogger('emberguard')
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = RotatingFileHandler(
        'logs/emberguard.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(
        logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(
        logging.Formatter('%(levelname)s: %(message)s')
    )
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# metrics.py
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
detection_counter = Counter(
    'fire_detections_total',
    'Total number of fire detections'
)

false_positive_counter = Counter(
    'false_positives_suppressed_total',
    'Total number of false positives suppressed'
)

inference_time = Histogram(
    'inference_duration_seconds',
    'Time spent on inference'
)

active_alerts = Gauge(
    'active_fire_alerts',
    'Number of active fire alerts'
)

# ä½¿ç”¨ç¤ºä¾‹
def process_with_metrics(frame):
    with inference_time.time():
        result = detector.process_frame(frame)
    
    if result and result.get('alert'):
        detection_counter.inc()
        active_alerts.inc()
    
    return result
```

#### é¢„æœŸæˆæœ
- âœ… å®Œæ•´çš„Webåº”ç”¨
- âœ… RESTful APIæ¥å£
- âœ… WebSocketå®æ—¶æ£€æµ‹
- âœ… Dockerä¸€é”®éƒ¨ç½²
- âœ… æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
- âœ… æ—¥å¿—è®°å½•å®Œå–„

---


## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ä¸è¯„ä¼°

### ç›®æ ‡æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | Phase 1 | Phase 2 | Phase 3 | Phase 4 |
|------|--------|---------|---------|---------|---------|
| **æ£€æµ‹å‡†ç¡®ç‡** | >99% | 90% | 95% | 97% | 99% |
| **è¯¯æŠ¥ç‡** | <2% | 10% | 5% | 2% | <2% |
| **æ¨ç†é€Ÿåº¦(GPU)** | >30 FPS | 45 FPS | 35 FPS | 30 FPS | 30 FPS |
| **æ¨ç†é€Ÿåº¦(CPU)** | >10 FPS | 8 FPS | 6 FPS | 5 FPS | 10 FPS |
| **æ¨¡å‹å¤§å°** | <50MB | 6MB | 15MB | 25MB | 20MB |
| **é¦–å¸§æ£€æµ‹æ—¶é—´** | <3s | 1s | 1.5s | 2s | 1s |

### è¯„ä¼°æ•°æ®é›†

#### è®­ç»ƒé›†
- **D-Fireæ•°æ®é›†**: 21,527å¼ å›¾åƒ
- **è‡ªé‡‡é›†æ•°æ®**: 5,000å¼ å›¾åƒ
- **æ•°æ®å¢å¼º**: 3xæ‰©å……
- **æ€»è®¡**: ~80,000å¼ å›¾åƒ

#### éªŒè¯é›†
- **D-FireéªŒè¯é›†**: 2,000å¼ å›¾åƒ
- **è‡ªé‡‡é›†éªŒè¯é›†**: 500å¼ å›¾åƒ
- **æ€»è®¡**: 2,500å¼ å›¾åƒ

#### æµ‹è¯•é›†
- **çœŸå®åœºæ™¯è§†é¢‘**: 100ä¸ª
  - å®¤å†…ç«ç¾: 30ä¸ª
  - å®¤å¤–ç«ç¾: 30ä¸ª
  - ç‚ŠçƒŸåœºæ™¯: 20ä¸ª
  - æ­£å¸¸åœºæ™¯: 20ä¸ª

### è¯„ä¼°æ–¹æ³•

```python
def evaluate_model(model, test_dataset):
    """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    
    metrics = {
        'accuracy': 0,
        'precision': 0,
        'recall': 0,
        'f1_score': 0,
        'false_positive_rate': 0,
        'false_negative_rate': 0,
        'inference_time': [],
        'confusion_matrix': None
    }
    
    y_true = []
    y_pred = []
    
    for video_path, label in test_dataset:
        start_time = time.time()
        
        # å¤„ç†è§†é¢‘
        predictions = process_video(model, video_path)
        
        # è®°å½•æ¨ç†æ—¶é—´
        inference_time = time.time() - start_time
        metrics['inference_time'].append(inference_time)
        
        # èšåˆé¢„æµ‹ç»“æœ
        final_pred = aggregate_predictions(predictions)
        
        y_true.append(label)
        y_pred.append(final_pred)
    
    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score,
        f1_score, confusion_matrix
    )
    
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred, average='weighted')
    metrics['recall'] = recall_score(y_true, y_pred, average='weighted')
    metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')
    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)
    
    # è®¡ç®—è¯¯æŠ¥ç‡å’Œæ¼æŠ¥ç‡
    tn, fp, fn, tp = metrics['confusion_matrix'].ravel()
    metrics['false_positive_rate'] = fp / (fp + tn)
    metrics['false_negative_rate'] = fn / (fn + tp)
    
    # å¹³å‡æ¨ç†æ—¶é—´
    metrics['avg_inference_time'] = np.mean(metrics['inference_time'])
    
    return metrics
```

---

## ğŸ”§ å¼€å‘å·¥å…·ä¸ç¯å¢ƒ

### å¼€å‘ç¯å¢ƒ
```bash
# Pythonç¯å¢ƒ
Python 3.11+
CUDA 11.8+
cuDNN 8.6+

# æ ¸å¿ƒä¾èµ–
ultralytics==8.3.0
torch==2.0.0
torchvision==0.15.0
opencv-python==4.8.0
tensorflow==2.13.0  # for LSTM
numpy==1.24.0
pandas==2.0.0
scikit-learn==1.3.0
scipy==1.11.0

# Webæ¡†æ¶
fastapi==0.104.0
streamlit==1.28.0
uvicorn==0.24.0

# å·¥å…·åº“
albumentations==1.3.0  # æ•°æ®å¢å¼º
plotly==5.17.0  # å¯è§†åŒ–
prometheus-client==0.18.0  # ç›‘æ§
```

### æ¨èç¡¬ä»¶é…ç½®

#### å¼€å‘ç¯å¢ƒ
- **CPU**: Intel i7/AMD Ryzen 7 æˆ–æ›´é«˜
- **GPU**: NVIDIA RTX 3060 (12GB) æˆ–æ›´é«˜
- **å†…å­˜**: 32GB RAM
- **å­˜å‚¨**: 500GB SSD

#### ç”Ÿäº§ç¯å¢ƒ
- **CPU**: Intel Xeon/AMD EPYC
- **GPU**: NVIDIA T4/A10 æˆ–æ›´é«˜
- **å†…å­˜**: 64GB RAM
- **å­˜å‚¨**: 1TB NVMe SSD

#### è¾¹ç¼˜è®¾å¤‡
- **Jetson Nano**: å…¥é—¨çº§è¾¹ç¼˜éƒ¨ç½²
- **Jetson Xavier NX**: æ¨èé…ç½®
- **Jetson AGX Orin**: é«˜æ€§èƒ½é…ç½®

---

## ğŸ“š å‚è€ƒèµ„æ–™

### å­¦æœ¯è®ºæ–‡
1. **YOLOv8**: Ultralytics YOLOv8 Documentation
2. **Fire Detection**: "A hybrid method for fire detection based on spatial and temporal patterns" (Neural Computing and Applications, 2023)
3. **STCNet**: "STCNet: Spatio-Temporal Cross Network for Industrial Smoke Detection" (arXiv:2011.04863)
4. **LSTM**: "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997)

### å¼€æºé¡¹ç›®
1. [sureshkumark23/yolo-lstm_fire_detection](https://github.com/sureshkumark23/yolo-lstm_fire_detection-in-cctv-videos)
2. [pedbrgs/Fire-Detection](https://github.com/pedbrgs/Fire-Detection)
3. [Caoyichao/STCNet](https://github.com/Caoyichao/STCNet)
4. [harmeshgv/YoloV8-LSTM-video-Classification](https://github.com/harmeshgv/YoloV8-LSTM-video-Classification)

### æ•°æ®é›†
1. **D-Fire Dataset**: [GitHub](https://github.com/gaiasd/DFireDataset)
2. **FireNet Dataset**: [Google Drive](https://drive.google.com/drive/folders/1HznoBFEd6yjaLFlSmkUGARwCUzzG4whq)
3. **Foggia's Dataset**: [MIVIA](https://mivia.unisa.it/datasets/video-analysis-datasets/fire-detection-dataset/)

### æŠ€æœ¯æ–‡æ¡£
1. [Ultralytics YOLOv8 Docs](https://docs.ultralytics.com/)
2. [PyTorch Documentation](https://pytorch.org/docs/)
3. [TensorFlow/Keras LSTM Guide](https://www.tensorflow.org/guide/keras/rnn)
4. [FastAPI Documentation](https://fastapi.tiangolo.com/)
5. [Streamlit Documentation](https://docs.streamlit.io/)

---

## ğŸ¯ æ€»ç»“ä¸å»ºè®®

### æ ¸å¿ƒæŠ€æœ¯è·¯çº¿

```
YOLOv8æ£€æµ‹ â†’ ç‰¹å¾æå–(16ç»´) â†’ LSTMæ—¶åºåˆ†æ â†’ è¯¯æŠ¥æŠ‘åˆ¶ â†’ çƒŸé›¾åˆ†ç±»
    â†“            â†“                  â†“              â†“            â†“
  D-Fire      å‡ ä½•+é¢œè‰²+è¿åŠ¨      æ»‘åŠ¨çª—å£      è¿½è¸ª+AVT/TPT   ç‚ŠçƒŸvsç«ç¾
```

### å…³é”®æˆåŠŸå› ç´ 

1. **æ•°æ®è´¨é‡** â­â­â­â­â­
   - é«˜è´¨é‡æ ‡æ³¨æ•°æ®
   - å¤šæ ·åŒ–åœºæ™¯è¦†ç›–
   - å……åˆ†çš„æ•°æ®å¢å¼º

2. **ç‰¹å¾å·¥ç¨‹** â­â­â­â­
   - 16ç»´ç»¼åˆç‰¹å¾
   - æ—¶åºç‰¹å¾æå–
   - å¤šæ¨¡æ€èåˆ

3. **è¯¯æŠ¥æ§åˆ¶** â­â­â­â­â­
   - ç›®æ ‡è¿½è¸ª
   - é¢ç§¯å˜åŒ–åˆ†æ
   - æ—¶åºæŒç»­æ€§æ£€æŸ¥
   - çƒŸé›¾åˆ†ç±»

4. **å·¥ç¨‹å®ç°** â­â­â­â­
   - æ¨¡å—åŒ–è®¾è®¡
   - å®Œå–„çš„API
   - Dockeréƒ¨ç½²
   - æ€§èƒ½ç›‘æ§

### é£é™©ä¸åº”å¯¹

| é£é™© | å½±å“ | åº”å¯¹æªæ–½ |
|------|------|----------|
| æ•°æ®ä¸è¶³ | é«˜ | æ•°æ®å¢å¼ºã€è¿ç§»å­¦ä¹  |
| è¯¯æŠ¥ç‡é«˜ | é«˜ | å¤šå±‚è¯¯æŠ¥æŠ‘åˆ¶æœºåˆ¶ |
| å®æ—¶æ€§å·® | ä¸­ | æ¨¡å‹é‡åŒ–ã€æ‰¹å¤„ç†ä¼˜åŒ– |
| ç‚ŠçƒŸè¯¯æŠ¥ | é«˜ | ä¸“é—¨çš„çƒŸé›¾åˆ†ç±»å™¨ |
| è¾¹ç¼˜éƒ¨ç½²éš¾ | ä¸­ | è½»é‡åŒ–æ¨¡å‹ã€TensorRTåŠ é€Ÿ |

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¼€å§‹** (æœ¬å‘¨)
   - âœ… æ­å»ºå¼€å‘ç¯å¢ƒ
   - âœ… å‡†å¤‡D-Fireæ•°æ®é›†
   - âœ… è®­ç»ƒåŸºç¡€YOLOv8æ¨¡å‹

2. **Phase 1å®æ–½** (ç¬¬1-2å‘¨)
   - å®ç°YOLO-LSTMåŸºç¡€æ¶æ„
   - å®Œæˆç‰¹å¾æå–å™¨
   - è®­ç»ƒLSTMæ¨¡å‹
   - æ„å»ºæ¨ç†ç®¡é“

3. **Phase 2å®æ–½** (ç¬¬3-4å‘¨)
   - æ‰©å±•ç‰¹å¾ç»´åº¦
   - å®ç°ç›®æ ‡è¿½è¸ª
   - æ·»åŠ è¯¯æŠ¥æŠ‘åˆ¶
   - æ€§èƒ½ä¼˜åŒ–

4. **Phase 3å®æ–½** (ç¬¬5-6å‘¨)
   - çƒŸé›¾ç‰¹å¾æå–
   - è®­ç»ƒçƒŸé›¾åˆ†ç±»å™¨
   - é›†æˆåˆ°ä¸»ç®¡é“
   - æµ‹è¯•ç‚ŠçƒŸåœºæ™¯

5. **Phase 4å®æ–½** (ç¬¬7-8å‘¨)
   - å¼€å‘Webåº”ç”¨
   - å®ç°APIæ¥å£
   - Dockeréƒ¨ç½²
   - æ€§èƒ½æµ‹è¯•

---

## ğŸ“ è”ç³»ä¸æ”¯æŒ

å¦‚æœ‰æŠ€æœ¯é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥è®¨è®ºï¼Œè¯·è”ç³»ï¼š

- **é¡¹ç›®è´Ÿè´£äºº**: EmberGuard Team
- **æŠ€æœ¯æ”¯æŒ**: [GitHub Issues](https://github.com/create-meng/EmberGuard-AI-train/issues)
- **æ–‡æ¡£æ›´æ–°**: 2026å¹´2æœˆ6æ—¥

---

**æ–‡æ¡£ç»“æŸ**

*æœ¬æŠ€æœ¯ç ”ç©¶æŠ¥å‘ŠåŸºäº4ä¸ªå¼€æºé¡¹ç›®çš„æ·±å…¥åˆ†æï¼Œæä¾›äº†å®Œæ•´çš„æŠ€æœ¯å®ç°è·¯çº¿å›¾ã€‚å»ºè®®æŒ‰ç…§Phase 1-4çš„é¡ºåºé€æ­¥å®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„ç›®æ ‡å’Œå¯äº¤ä»˜æˆæœã€‚*
