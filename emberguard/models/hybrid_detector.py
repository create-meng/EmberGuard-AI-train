"""
æ··åˆæ£€æµ‹å™¨ - æ•´åˆYOLOå’ŒLSTM
"""

import cv2
import numpy as np
from collections import deque
from typing import Tuple, Optional
from .yolo_detector import YOLOFireDetector
from .lstm_classifier import LSTMFireClassifier


class HybridFireDetector:
    """
    YOLO + LSTM æ··åˆç«ç¾æ£€æµ‹å™¨
    ç»“åˆç©ºé—´ç‰¹å¾ï¼ˆYOLOï¼‰å’Œæ—¶åºç‰¹å¾ï¼ˆLSTMï¼‰è¿›è¡Œå‡†ç¡®æ£€æµ‹
    """
    
    def __init__(self, yolo_model_path: str, lstm_model_path: str = None,
                 seq_length: int = 30, conf_threshold: float = 0.25):
        """
        åˆå§‹åŒ–æ··åˆæ£€æµ‹å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
            lstm_model_path: LSTMæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            seq_length: LSTMåºåˆ—é•¿åº¦
            conf_threshold: YOLOç½®ä¿¡åº¦é˜ˆå€¼
        """
        # åˆå§‹åŒ–YOLOæ£€æµ‹å™¨
        self.yolo_detector = YOLOFireDetector(yolo_model_path, conf_threshold)
        
        # åˆå§‹åŒ–LSTMåˆ†ç±»å™¨
        self.lstm_classifier = LSTMFireClassifier(lstm_model_path, seq_length)
        
        # ç‰¹å¾ç¼“å†²åŒº - å­˜å‚¨æœ€è¿‘çš„ç‰¹å¾åºåˆ—
        self.feature_buffer = deque(maxlen=seq_length)
        
        # æ£€æµ‹çŠ¶æ€
        self.current_prediction = "no_fire"
        self.current_confidence = 0.0
        
    def process_frame(self, frame: np.ndarray, use_lstm: bool = True) -> Tuple[np.ndarray, dict]:
        """
        å¤„ç†å•å¸§å›¾åƒ
        
        Args:
            frame: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            use_lstm: æ˜¯å¦ä½¿ç”¨LSTMè¿›è¡Œæ—¶åºåˆ†æ
            
        Returns:
            (æ ‡æ³¨åçš„å›¾åƒ, æ£€æµ‹ç»“æœå­—å…¸)
        """
        # Step 1: YOLOæ£€æµ‹
        detections = self.yolo_detector.detect(frame)
        
        # Step 2: æå–ç‰¹å¾å‘é‡
        features = self.yolo_detector.extract_features_vector(detections)
        self.feature_buffer.append(features)
        
        # Step 3: LSTMæ—¶åºåˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”ç¼“å†²åŒºè¶³å¤Ÿï¼‰
        if use_lstm and len(self.feature_buffer) >= self.lstm_classifier.seq_length:
            feature_seq = np.array(list(self.feature_buffer))
            pred_class, pred_conf, all_probs = self.lstm_classifier.predict(feature_seq)
            
            self.current_prediction = pred_class
            self.current_confidence = pred_conf
        else:
            # ä»…ä½¿ç”¨YOLOç»“æœ
            if detections:
                best_det = max(detections, key=lambda x: x['confidence'])
                self.current_prediction = best_det['class_name']
                self.current_confidence = best_det['confidence']
            else:
                self.current_prediction = "no_fire"
                self.current_confidence = 0.0
        
        # Step 4: ç»˜åˆ¶æ£€æµ‹ç»“æœ
        annotated_frame = self.yolo_detector.draw_detections(frame, detections)
        
        # æ·»åŠ LSTMé¢„æµ‹ç»“æœ
        self._draw_lstm_prediction(annotated_frame)
        
        # æ„å»ºç»“æœå­—å…¸
        result = {
            'yolo_detections': detections,
            'lstm_prediction': self.current_prediction,
            'lstm_confidence': self.current_confidence,
            'buffer_size': len(self.feature_buffer),
            'is_fire_detected': self.current_prediction in ['fire', 'smoke']
        }
        
        return annotated_frame, result
    
    def _draw_lstm_prediction(self, frame: np.ndarray):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶LSTMé¢„æµ‹ç»“æœ
        
        Args:
            frame: è¾“å…¥å›¾åƒï¼ˆä¼šè¢«ç›´æ¥ä¿®æ”¹ï¼‰
        """
        # æ ¹æ®é¢„æµ‹ç»“æœé€‰æ‹©é¢œè‰²
        if self.current_prediction == 'fire':
            color = (0, 0, 255)  # çº¢è‰²
            emoji = "ğŸ”¥"
        elif self.current_prediction == 'smoke':
            color = (0, 255, 255)  # é»„è‰²
            emoji = "ğŸ’¨"
        else:
            color = (0, 255, 0)  # ç»¿è‰²
            emoji = "âœ…"
        
        # ç»˜åˆ¶é¢„æµ‹ç»“æœ
        text = f"{emoji} LSTM: {self.current_prediction.upper()} ({self.current_confidence:.2f})"
        cv2.putText(frame, text, (30, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 3)
        
        # ç»˜åˆ¶ç¼“å†²åŒºçŠ¶æ€
        buffer_text = f"Buffer: {len(self.feature_buffer)}/{self.lstm_classifier.seq_length}"
        cv2.putText(frame, buffer_text, (30, 90),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    def reset_buffer(self):
        """é‡ç½®ç‰¹å¾ç¼“å†²åŒº"""
        self.feature_buffer.clear()
        self.current_prediction = "no_fire"
        self.current_confidence = 0.0
    
    def process_video(self, video_path: str, output_path: Optional[str] = None,
                     display: bool = True) -> dict:
        """
        å¤„ç†è§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            display: æ˜¯å¦æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        cap = cv2.VideoCapture(video_path)
        
        # è·å–è§†é¢‘å±æ€§
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        writer = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_frames': total_frames,
            'fire_frames': 0,
            'smoke_frames': 0,
            'no_fire_frames': 0
        }
        
        frame_count = 0
        
        print(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}, FPS: {fps}, æ€»å¸§æ•°: {total_frames}")
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # å¤„ç†å¸§
            annotated_frame, result = self.process_frame(frame)
            
            # æ›´æ–°ç»Ÿè®¡
            if result['lstm_prediction'] == 'fire':
                stats['fire_frames'] += 1
            elif result['lstm_prediction'] == 'smoke':
                stats['smoke_frames'] += 1
            else:
                stats['no_fire_frames'] += 1
            
            # å†™å…¥è¾“å‡ºè§†é¢‘
            if writer:
                writer.write(annotated_frame)
            
            # æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹
            if display:
                cv2.imshow("EmberGuard AI - Fire Detection", annotated_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("âš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
                    break
            
            frame_count += 1
            if frame_count % 30 == 0:
                print(f"   å¤„ç†è¿›åº¦: {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)")
        
        # æ¸…ç†èµ„æº
        cap.release()
        if writer:
            writer.release()
        if display:
            cv2.destroyAllWindows()
        
        print(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ!")
        print(f"   ç«ç„°å¸§: {stats['fire_frames']}, çƒŸé›¾å¸§: {stats['smoke_frames']}, æ­£å¸¸å¸§: {stats['no_fire_frames']}")
        
        return stats
    
    def process_webcam(self, camera_id: int = 0):
        """
        å¤„ç†æ‘„åƒå¤´å®æ—¶è§†é¢‘æµ
        
        Args:
            camera_id: æ‘„åƒå¤´IDï¼ˆé»˜è®¤0ä¸ºä¸»æ‘„åƒå¤´ï¼‰
        """
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")
            return
        
        print(f"ğŸ“¹ å¼€å§‹å®æ—¶æ£€æµ‹ (æ‘„åƒå¤´ {camera_id})")
        print("   æŒ‰ 'q' é€€å‡º, æŒ‰ 'r' é‡ç½®ç¼“å†²åŒº")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                break
            
            # å¤„ç†å¸§
            annotated_frame, result = self.process_frame(frame)
            
            # æ˜¾ç¤ºç»“æœ
            cv2.imshow("EmberGuard AI - Live Detection", annotated_frame)
            
            # é”®ç›˜æ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('r'):
                self.reset_buffer()
                print("ğŸ”„ å·²é‡ç½®ç‰¹å¾ç¼“å†²åŒº")
        
        cap.release()
        cv2.destroyAllWindows()
        print("âœ… å®æ—¶æ£€æµ‹ç»“æŸ")
